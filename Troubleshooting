[root@mgmt-a087 ~]# for i in mon1 mon2 mon3 client1 node1 node2 node3 node4; do ssh $i date ; done
dom mar  5 16:57:24 EST 2017
dom mar  5 16:57:24 EST 2017
dom mar  5 16:57:24 EST 2017
dom mar  5 16:57:24 EST 2017
dom mar  5 16:57:24 EST 2017
dom mar  5 16:57:25 EST 2017
dom mar  5 16:57:25 EST 2017
dom mar  5 16:57:25 EST 2017
[root@mgmt-a087 ~]# ceph -s
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_WARN
            30 pgs peering
            30 pgs stuck inactive
            30 pgs stuck unclean
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e32: 3 osds: 3 up, 3 in
      pgmap v55: 64 pgs, 1 pools, 0 bytes data, 0 objects
            102224 kB used, 27515 MB / 27614 MB avail
                  34 active+clean
                  30 peering
[root@mgmt-a087 ~]# ceph -w
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_WARN
            30 pgs peering
            30 pgs stuck inactive
            30 pgs stuck unclean
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e32: 3 osds: 3 up, 3 in
      pgmap v55: 64 pgs, 1 pools, 0 bytes data, 0 objects
            102224 kB used, 27515 MB / 27614 MB avail
                  34 active+clean
                  30 peering

2017-03-05 16:40:19.605511 mon.0 [INF] pgmap v55: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
^C[root@mgmt-a087 ~]# ntpstat 
synchronised to NTP server (199.102.46.80) at stratum 2 
   time correct to within 14 ms
   polling server every 64 s
[root@mgmt-a087 ~]# ssh mon1
Last login: Sun Mar  5 16:36:00 2017 from mgmt-a087.rhpds.opentlc.com
[root@mon1 ~]# ceph -s
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_WARN
            30 pgs peering
            30 pgs stuck inactive
            30 pgs stuck unclean
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e32: 3 osds: 3 up, 3 in
      pgmap v55: 64 pgs, 1 pools, 0 bytes data, 0 objects
            102224 kB used, 27515 MB / 27614 MB avail
                  34 active+clean
                  30 peering
[root@mon1 ~]# cat /var/log/ceph/ceph
ceph.audit.log     ceph.log           ceph-mon.mon1.log  
[root@mon1 ~]# cat /var/log/ceph/ceph.audit.log 
2017-03-05 15:19:36.677540 mon.1 10.100.2.12:6789/0 38 : audit [INF] from='client.? 10.100.2.12:0/1008928' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.680288 mon.0 10.100.2.11:6789/0 60 : audit [INF] from='client.4099 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.687102 mon.0 10.100.2.11:6789/0 61 : audit [INF] from='client.4099 :/0' entity='mon.' cmd='[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]': finished
2017-03-05 15:19:37.215851 mon.2 10.100.2.13:6789/0 27 : audit [INF] from='client.? 10.100.2.12:0/1008963' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.217753 mon.0 10.100.2.11:6789/0 62 : audit [INF] from='client.4100 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.238164 mon.0 10.100.2.11:6789/0 63 : audit [INF] from='client.4100 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]': finished
2017-03-05 15:19:37.539069 mon.1 10.100.2.12:6789/0 39 : audit [INF] from='client.? 10.100.2.13:0/1008929' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.541079 mon.0 10.100.2.11:6789/0 64 : audit [INF] from='client.4102 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.719385 mon.2 10.100.2.13:6789/0 28 : audit [INF] from='client.? 10.100.2.12:0/1008996' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.721040 mon.0 10.100.2.11:6789/0 65 : audit [INF] from='client.4103 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.745071 mon.0 10.100.2.11:6789/0 66 : audit [INF] from='client.4103 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]': finished
2017-03-05 15:19:37.945696 mon.2 10.100.2.13:6789/0 29 : audit [INF] from='client.? 10.100.2.13:0/1008964' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.947374 mon.0 10.100.2.11:6789/0 67 : audit [INF] from='client.4106 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:38.173084 mon.1 10.100.2.12:6789/0 40 : audit [INF] from='client.? 10.100.2.12:0/1009029' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.175433 mon.0 10.100.2.11:6789/0 68 : audit [INF] from='client.4105 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.198421 mon.0 10.100.2.11:6789/0 69 : audit [INF] from='client.4105 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]': finished
2017-03-05 15:19:38.377593 mon.1 10.100.2.12:6789/0 41 : audit [INF] from='client.? 10.100.2.13:0/1008998' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.379691 mon.0 10.100.2.11:6789/0 70 : audit [INF] from='client.4108 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.777147 mon.1 10.100.2.12:6789/0 42 : audit [INF] from='client.? 10.100.2.13:0/1009030' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.778896 mon.0 10.100.2.11:6789/0 71 : audit [INF] from='client.4111 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:39.448305 mon.1 10.100.2.12:6789/0 43 : audit [INF] from='client.? 10.100.2.11:0/1008970' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.449979 mon.0 10.100.2.11:6789/0 72 : audit [INF] from='client.4114 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.853054 mon.0 10.100.2.11:6789/0 73 : audit [INF] from='client.? 10.100.2.11:0/1009005' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:40.264364 mon.2 10.100.2.13:6789/0 30 : audit [INF] from='client.? 10.100.2.11:0/1009039' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.265910 mon.0 10.100.2.11:6789/0 74 : audit [INF] from='client.4109 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.670360 mon.2 10.100.2.13:6789/0 31 : audit [INF] from='client.? 10.100.2.11:0/1009071' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:40.671760 mon.0 10.100.2.11:6789/0 75 : audit [INF] from='client.4112 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:26:13.363578 mon.2 10.100.2.13:6789/0 34 : audit [INF] from='client.? 10.100.1.11:0/1008847' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.365838 mon.0 10.100.2.11:6789/0 79 : audit [INF] from='client.4115 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.373753 mon.0 10.100.2.11:6789/0 80 : audit [INF] from='client.4115 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]': finished
2017-03-05 15:26:14.501535 mon.2 10.100.2.13:6789/0 36 : audit [INF] from='client.? 10.100.1.11:0/1008938' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.503751 mon.0 10.100.2.11:6789/0 83 : audit [INF] from='client.4121 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.509119 mon.0 10.100.2.11:6789/0 84 : audit [INF] from='client.4121 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:15.792232 mon.0 10.100.2.11:6789/0 85 : audit [INF] from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 15:26:16.462114 mon.0 10.100.2.11:6789/0 86 : audit [INF] from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]': finished
2017-03-05 15:26:24.405756 mon.1 10.100.2.12:6789/0 46 : audit [INF] from='client.? 10.100.1.12:0/1008833' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.408008 mon.0 10.100.2.11:6789/0 95 : audit [INF] from='client.4120 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.988022 mon.0 10.100.2.11:6789/0 96 : audit [INF] from='client.4120 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]': finished
2017-03-05 15:26:26.058721 mon.1 10.100.2.12:6789/0 48 : audit [INF] from='client.? 10.100.1.12:0/1008923' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.061044 mon.0 10.100.2.11:6789/0 99 : audit [INF] from='client.4126 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.066846 mon.0 10.100.2.11:6789/0 100 : audit [INF] from='client.4126 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:27.511254 mon.2 10.100.2.13:6789/0 37 : audit [INF] from='client.? 10.100.1.12:0/1009091' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:27.513219 mon.0 10.100.2.11:6789/0 101 : audit [INF] from='client.4124 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:28.012613 mon.0 10.100.2.11:6789/0 102 : audit [INF] from='client.4124 :/0' entity='osd.1' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]': finished
2017-03-05 15:26:35.709195 mon.0 10.100.2.11:6789/0 112 : audit [INF] from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]: dispatch
2017-03-05 15:26:36.659122 mon.0 10.100.2.11:6789/0 114 : audit [INF] from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]': finished
2017-03-05 15:26:37.733812 mon.0 10.100.2.11:6789/0 118 : audit [INF] from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:37.739715 mon.0 10.100.2.11:6789/0 119 : audit [INF] from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:39.015598 mon.1 10.100.2.12:6789/0 49 : audit [INF] from='client.? 10.100.1.13:0/1009103' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.017825 mon.0 10.100.2.11:6789/0 120 : audit [INF] from='client.4129 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.693051 mon.0 10.100.2.11:6789/0 121 : audit [INF] from='client.4129 :/0' entity='osd.2' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]': finished
2017-03-05 16:17:21.549509 mon.2 10.100.2.13:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1000896' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.552062 mon.0 10.100.2.11:6789/0 16 : audit [INF] from='client.24101 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.793832 mon.2 10.100.2.13:6789/0 7 : audit [INF] from='client.? 10.100.1.13:0/1001061' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.795145 mon.0 10.100.2.11:6789/0 17 : audit [INF] from='client.24104 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.978268 mon.0 10.100.2.11:6789/0 18 : audit [INF] from='client.24100 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:22.087369 mon.1 10.100.2.12:6789/0 4 : audit [INF] from='client.? 10.100.1.11:0/1000920' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.637410 mon.0 10.100.2.11:6789/0 21 : audit [INF] from='client.? 10.100.1.12:0/1001118' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.680179 mon.2 10.100.2.13:6789/0 8 : audit [INF] from='client.? 10.100.1.13:0/1000892' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.681455 mon.0 10.100.2.11:6789/0 22 : audit [INF] from='client.24110 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.811218 mon.0 10.100.2.11:6789/0 23 : audit [INF] from='client.24103 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.919879 mon.1 10.100.2.12:6789/0 5 : audit [INF] from='client.? 10.100.1.11:0/1001122' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.786703 mon.2 10.100.2.13:6789/0 9 : audit [INF] from='client.? 10.100.1.12:0/1001444' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.787891 mon.0 10.100.2.11:6789/0 28 : audit [INF] from='client.24113 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.970634 mon.2 10.100.2.13:6789/0 10 : audit [INF] from='client.? 10.100.1.13:0/1001418' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.971829 mon.0 10.100.2.11:6789/0 29 : audit [INF] from='client.24116 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.992162 mon.2 10.100.2.13:6789/0 11 : audit [INF] from='client.? 10.100.1.11:0/1001438' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.993256 mon.0 10.100.2.11:6789/0 30 : audit [INF] from='client.24119 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.088951 mon.0 10.100.2.11:6789/0 35 : audit [INF] from='client.? 10.100.1.12:0/1001666' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.251556 mon.2 10.100.2.13:6789/0 12 : audit [INF] from='client.? 10.100.1.13:0/1001640' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.253025 mon.0 10.100.2.11:6789/0 36 : audit [INF] from='client.24122 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.295461 mon.0 10.100.2.11:6789/0 37 : audit [INF] from='client.? 10.100.1.11:0/1001660' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.025454 mon.1 10.100.2.12:6789/0 4 : audit [INF] from='client.? 10.100.1.13:0/1001084' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.027686 mon.0 10.100.2.11:6789/0 10 : audit [INF] from='client.34099 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.014671 mon.2 10.100.2.13:6789/0 4 : audit [INF] from='client.? 10.100.1.13:0/1000893' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.016537 mon.0 10.100.2.11:6789/0 11 : audit [INF] from='client.34103 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.313298 mon.2 10.100.2.13:6789/0 5 : audit [INF] from='client.? 10.100.1.13:0/1001417' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.314827 mon.0 10.100.2.11:6789/0 12 : audit [INF] from='client.34106 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:52.533982 mon.0 10.100.2.11:6789/0 13 : audit [INF] from='client.? 10.100.1.13:0/1001617' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.539484 mon.1 10.100.2.12:6789/0 5 : audit [INF] from='client.? 10.100.1.12:0/1000925' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.541498 mon.0 10.100.2.11:6789/0 14 : audit [INF] from='client.34102 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.247140 mon.2 10.100.2.13:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1001120' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.248913 mon.0 10.100.2.11:6789/0 15 : audit [INF] from='client.34109 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:07.570939 mon.0 10.100.2.11:6789/0 18 : audit [INF] from='client.? 10.100.1.12:0/1001416' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.910830 mon.1 10.100.2.12:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1001622' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.912985 mon.0 10.100.2.11:6789/0 22 : audit [INF] from='client.34105 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.278050 mon.1 10.100.2.12:6789/0 7 : audit [INF] from='client.? 10.100.1.11:0/1000877' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.280094 mon.0 10.100.2.11:6789/0 23 : audit [INF] from='client.34108 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:11.682610 mon.0 10.100.2.11:6789/0 26 : audit [INF] from='client.? 10.100.1.11:0/1001155' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.172357 mon.2 10.100.2.13:6789/0 7 : audit [INF] from='client.? 10.100.1.11:0/1001508' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.174223 mon.0 10.100.2.11:6789/0 30 : audit [INF] from='client.34115 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# cat /var/log/ceph/ceph.
ceph.audit.log  ceph.log        
[root@mon1 ~]# cat /var/log/ceph/ceph
ceph.audit.log     ceph.log           ceph-mon.mon1.log  
[root@mon1 ~]# cat /var/log/ceph/ceph-mon.mon1.log 
2017-03-05 15:19:15.193363 7f11ef2ce7c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 8786
2017-03-05 15:19:15.365268 7f11ef2ce7c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:19:15.366316 7f11ef2ce7c0  1 mon.mon1@-1(probing) e0 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:19:15.366439 7f11ef2ce7c0  1 mon.mon1@-1(probing) e0  initial_members mon1,mon2,mon3, filtering seed monmap
2017-03-05 15:19:15.369379 7f11ef2ce7c0  0 mon.mon1@-1(probing) e0  my rank is now 0 (was -1)
2017-03-05 15:19:15.379863 7f11e6915700  0 -- 10.100.2.11:6789/0 >> 0.0.0.0:0/2 pipe(0x42e1000 sd=23 :0 s=1 pgs=0 cs=0 l=0 c=0x417bce0).fault
2017-03-05 15:19:15.380295 7f11e6713700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42eb000 sd=20 :0 s=1 pgs=0 cs=0 l=0 c=0x417c260).fault
2017-03-05 15:19:15.390886 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x42e6000 sd=22 :0 s=1 pgs=0 cs=0 l=0 c=0x417bfa0).fault
2017-03-05 15:19:15.391354 7f11ef2bc700  0 -- 10.100.2.11:6789/0 >> 0.0.0.0:0/1 pipe(0x42dc000 sd=21 :0 s=1 pgs=0 cs=0 l=0 c=0x417ba20).fault
2017-03-05 15:19:15.644907 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:15.645105 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:16.853956 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:16.854096 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.159597 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.159747 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.221830 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.221993 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.462388 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.462634 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:19.396688 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:19.396818 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:20.583908 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:20.584042 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:20.626451 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4320000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c680).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 15:19:20.678446 7f11e8118700  1 mon.mon1@0(probing) e0  adding peer 10.100.2.12:6789/0 to list of hints
2017-03-05 15:19:20.678518 7f11e8118700  1 mon.mon1@0(probing) e0  learned initial mon mon2 addr 10.100.2.12:6789/0
2017-03-05 15:19:20.679020 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:20.679151 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:20.684199 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x42e6000 sd=22 :36357 s=2 pgs=8 cs=1 l=0 c=0x417bfa0).fault, initiating reconnect
2017-03-05 15:19:20.712291 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x433a000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c940).accept connect_seq 0 vs existing 2 state connecting
2017-03-05 15:19:20.712360 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x433a000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c940).accept peer reset, then tried to connect to us, replacing
2017-03-05 15:19:21.770592 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:21.770737 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:22.960185 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:22.960377 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:24.151753 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:24.151894 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:25.333645 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:25.333818 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:25.682039 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:25.682136 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:25.844137 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x433f000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cc00).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 15:19:25.896087 7f11e8118700  1 mon.mon1@0(electing) e0  adding peer 10.100.2.13:6789/0 to list of hints
2017-03-05 15:19:25.896131 7f11e8118700  1 mon.mon1@0(electing) e0  learned initial mon mon3 addr 10.100.2.13:6789/0
2017-03-05 15:19:25.896795 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:25.896885 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:25.907650 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42eb000 sd=20 :50940 s=2 pgs=7 cs=1 l=0 c=0x417c260).fault, initiating reconnect
2017-03-05 15:19:25.927681 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cd60).accept connect_seq 0 vs existing 2 state connecting
2017-03-05 15:19:25.927709 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cd60).accept peer reset, then tried to connect to us, replacing
2017-03-05 15:19:26.517981 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:26.518116 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:27.698725 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:27.698855 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:28.874882 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:28.875021 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:29.886179 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:29.886406 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:30.067277 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:30.067404 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:30.900094 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:30.900189 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:31.255849 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:31.255983 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:32.436712 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:32.436930 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:33.618838 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:33.618962 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:34.782511 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:34.782680 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:35.094800 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:35.094940 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:35.913846 7f11e8919700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,2
2017-03-05 15:19:35.924230 7f11e8919700  1 mon.mon1@0(leader) e0 apply_quorum_to_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code}
2017-03-05 15:19:35.947782 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:35.947850 7f11e8118700  1 mon.mon1@0(electing).elector(2) init, last seen epoch 2
2017-03-05 15:19:35.976994 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:35.980006 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:35.980088 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:36.049002 7f11e991b700  1 mon.mon1@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.049034 7f11e991b700  1 mon.mon1@0(leader).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 15:19:36.061755 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.075818 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:36.075920 7f11e8118700  1 mon.mon1@0(electing).elector(4) init, last seen epoch 4
2017-03-05 15:19:36.123325 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:36.136861 7f11e8118700  0 log_channel(cluster) log [INF] : HEALTH_ERR; no osds
2017-03-05 15:19:36.187014 7f11e8118700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:19:36.236729 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.236906 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v1: 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:19:36.301056 7f11e991b700  0 mon.mon1@0(leader).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 15:19:36.301332 7f11e991b700  1 mon.mon1@0(leader).osd e1 e1: 0 osds: 0 up, 0 in
2017-03-05 15:19:36.303472 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303479 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303484 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303488 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.304186 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 1
2017-03-05 15:19:36.305385 7f11e991b700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 15:19:36.312205 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e1: 0 osds: 0 up, 0 in
2017-03-05 15:19:36.377879 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v2: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:19:36.680185 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:36.680282 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4099 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.687097 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4099 :/0' entity='mon.' cmd='[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]': finished
2017-03-05 15:19:37.217716 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:37.217749 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4100 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.238159 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4100 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]': finished
2017-03-05 15:19:37.541037 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:37.541074 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4102 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.721005 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:37.721036 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4103 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.745015 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4103 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]': finished
2017-03-05 15:19:37.947338 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:37.947370 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4106 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:38.175373 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:38.175422 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4105 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.198416 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4105 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]': finished
2017-03-05 15:19:38.379651 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:38.379687 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4108 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.778841 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:38.778887 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4111 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:39.449922 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:39.449970 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4114 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.853007 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:39.853046 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.2.11:0/1009005' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:40.265875 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:40.265905 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4109 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.671722 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:40.671755 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4112 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:45.343646 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:45.343777 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:20:15.367428 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1687 MB, avail 4446 MB
2017-03-05 15:20:36.137137 7f11e8919700  0 log_channel(cluster) log [INF] : HEALTH_ERR; 64 pgs stuck inactive; 64 pgs stuck unclean; no osds
2017-03-05 15:21:15.368497 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 71% total 6134 MB, used 1741 MB, avail 4392 MB
2017-03-05 15:22:15.369366 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:23:15.370106 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:24:15.370872 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:25:15.371498 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:26:13.365679 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"} v 0) v1
2017-03-05 15:26:13.365832 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4115 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.372625 7f11e991b700  1 mon.mon1@0(leader).osd e2 e2: 1 osds: 0 up, 0 in
2017-03-05 15:26:13.373741 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4115 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]': finished
2017-03-05 15:26:13.375167 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e2: 1 osds: 0 up, 0 in
2017-03-05 15:26:13.378881 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v3: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:14.503704 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:14.503746 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4121 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.509114 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4121 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:15.372087 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:26:15.792154 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 15:26:15.792226 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 15:26:15.792434 7f11e8118700  0 mon.mon1@0(leader).osd e2 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 15:26:16.460129 7f11e991b700  1 mon.mon1@0(leader).osd e3 e3: 1 osds: 0 up, 0 in
2017-03-05 15:26:16.462107 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]': finished
2017-03-05 15:26:16.463453 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e3: 1 osds: 0 up, 0 in
2017-03-05 15:26:16.466949 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v4: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:17.477868 7f11e991b700  1 mon.mon1@0(leader).osd e4 e4: 1 osds: 1 up, 1 in
2017-03-05 15:26:17.479796 7f11e991b700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/9152 boot
2017-03-05 15:26:17.481317 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e4: 1 osds: 1 up, 1 in
2017-03-05 15:26:17.485166 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v5: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:18.495168 7f11e991b700  1 mon.mon1@0(leader).osd e5 e5: 1 osds: 1 up, 1 in
2017-03-05 15:26:18.498435 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e5: 1 osds: 1 up, 1 in
2017-03-05 15:26:18.503438 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v6: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:22.964000 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v7: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:24.407956 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"} v 0) v1
2017-03-05 15:26:24.408003 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4120 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.986891 7f11e991b700  1 mon.mon1@0(leader).osd e6 e6: 2 osds: 1 up, 1 in
2017-03-05 15:26:24.988017 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4120 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]': finished
2017-03-05 15:26:24.990543 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e6: 2 osds: 1 up, 1 in
2017-03-05 15:26:24.994275 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v8: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:26.060982 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:26.061039 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4126 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.066841 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4126 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:27.513149 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 15:26:27.513213 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4124 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:27.513380 7f11e8118700  0 mon.mon1@0(leader).osd e6 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 15:26:28.011015 7f11e991b700  1 mon.mon1@0(leader).osd e7 e7: 2 osds: 1 up, 1 in
2017-03-05 15:26:28.012607 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4124 :/0' entity='osd.1' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]': finished
2017-03-05 15:26:28.015177 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e7: 2 osds: 1 up, 1 in
2017-03-05 15:26:28.021427 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v9: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:29.037826 7f11e991b700  1 mon.mon1@0(leader).osd e8 e8: 2 osds: 2 up, 2 in
2017-03-05 15:26:29.038850 7f11e991b700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/9137 boot
2017-03-05 15:26:29.040541 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e8: 2 osds: 2 up, 2 in
2017-03-05 15:26:29.049311 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v10: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:30.054978 7f11e991b700  1 mon.mon1@0(leader).osd e9 e9: 2 osds: 2 up, 2 in
2017-03-05 15:26:30.057323 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e9: 2 osds: 2 up, 2 in
2017-03-05 15:26:30.063746 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v11: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:34.635212 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v12: 64 pgs: 31 active+undersized+degraded, 33 active+clean; 0 bytes data, 68064 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:35.651446 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v13: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:35.709141 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"} v 0) v1
2017-03-05 15:26:35.709190 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]: dispatch
2017-03-05 15:26:36.140343 7f11e8919700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 15:26:36.657756 7f11e991b700  1 mon.mon1@0(leader).osd e10 e10: 3 osds: 2 up, 2 in
2017-03-05 15:26:36.659117 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]': finished
2017-03-05 15:26:36.660933 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e10: 3 osds: 2 up, 2 in
2017-03-05 15:26:36.667960 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v14: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:37.061120 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "mon getmap"} v 0) v1
2017-03-05 15:26:37.061162 7f11e8118700  0 log_channel(audit) log [DBG] : from='client.? 10.100.1.13:0/1008877' entity='client.bootstrap-osd' cmd=[{"prefix": "mon getmap"}]: dispatch
2017-03-05 15:26:37.733762 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:37.733807 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:37.739710 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:39.017749 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 15:26:39.017816 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4129 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.017946 7f11e8118700  0 mon.mon1@0(leader).osd e10 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 15:26:39.691814 7f11e991b700  1 mon.mon1@0(leader).osd e11 e11: 3 osds: 2 up, 2 in
2017-03-05 15:26:39.693046 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4129 :/0' entity='osd.2' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]': finished
2017-03-05 15:26:39.695079 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e11: 3 osds: 2 up, 2 in
2017-03-05 15:26:39.701775 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v15: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:40.699678 7f11e991b700  1 mon.mon1@0(leader).osd e12 e12: 3 osds: 3 up, 3 in
2017-03-05 15:26:40.700949 7f11e991b700  0 log_channel(cluster) log [INF] : osd.2 10.100.1.13:6800/9177 boot
2017-03-05 15:26:40.702990 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e12: 3 osds: 3 up, 3 in
2017-03-05 15:26:40.712102 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v16: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:41.718656 7f11e991b700  1 mon.mon1@0(leader).osd e13 e13: 3 osds: 3 up, 3 in
2017-03-05 15:26:41.721312 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e13: 3 osds: 3 up, 3 in
2017-03-05 15:26:41.726744 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v17: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:45.966382 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v18: 64 pgs: 64 active+clean; 0 bytes data, 68500 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:46.983972 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v19: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:27:15.372810 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:28:15.373583 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:28:46.011587 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v20: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:28:47.017419 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v21: 64 pgs: 64 active+clean; 0 bytes data, 102032 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:29:15.374399 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:30:15.375045 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:31:01.556835 7f11e8118700  0 log_channel(cluster) log [INF] : osd.1 marked itself down
2017-03-05 15:31:01.614633 7f11e991b700  1 mon.mon1@0(leader).osd e14 e14: 3 osds: 2 up, 3 in
2017-03-05 15:31:01.620221 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e14: 3 osds: 2 up, 3 in
2017-03-05 15:31:01.643642 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v22: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 102032 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:31:01.740238 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=2 pgs=10 cs=1 l=0 c=0x417c260).fault with nothing to send, going to standby
2017-03-05 15:31:01.946374 7f11e8118700  0 log_channel(cluster) log [INF] : osd.0 marked itself down
2017-03-05 15:31:02.279173 7f11e6612700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 15:31:02.279231 7f11e6612700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 15:31:02.279654 7f11e6612700  0 quorum service shutdown
2017-03-05 15:31:02.279658 7f11e6612700  0 mon.mon1@0(shutdown).health(6) HealthMonitor::service_shutdown 1 services
2017-03-05 15:31:02.279663 7f11e6612700  0 quorum service shutdown
2017-03-05 15:49:29.981794 7f71046cd7c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 837
2017-03-05 15:49:30.922367 7f71046cd7c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:49:30.923368 7f71046cd7c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:49:30.924134 7f71046cd7c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..22) refresh upgraded, format 0 -> 1
2017-03-05 15:49:30.924180 7f71046cd7c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 15:49:30.925195 7f71046cd7c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 15:49:30.925767 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925787 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925791 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925794 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.928566 7f71046cd7c0  1 mon.mon1@-1(probing).paxosservice(auth 1..9) refresh upgraded, format 0 -> 1
2017-03-05 15:49:30.931241 7f71046cd7c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 15:49:31.079312 7f70fd517700  1 mon.mon1@0(synchronizing).osd e15 e15: 3 osds: 1 up, 3 in
2017-03-05 15:49:31.079480 7f70fd517700  1 mon.mon1@0(synchronizing).osd e16 e16: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079532 7f70fd517700  1 mon.mon1@0(synchronizing).osd e17 e17: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079570 7f70fd517700  1 mon.mon1@0(synchronizing).osd e18 e18: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079603 7f70fd517700  1 mon.mon1@0(synchronizing).osd e19 e19: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079728 7f70fd517700  1 mon.mon1@0(synchronizing).osd e20 e20: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079777 7f70fd517700  1 mon.mon1@0(synchronizing).osd e21 e21: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079817 7f70fd517700  1 mon.mon1@0(synchronizing).osd e22 e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.083702 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:49:31.083820 7f70fd517700  1 mon.mon1@0(electing).elector(6) init, last seen epoch 6
2017-03-05 15:49:31.088928 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:49:31.089038 7f70fd517700  1 mon.mon1@0(electing).elector(9) init, last seen epoch 9
2017-03-05 15:49:31.090134 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:49:31.093502 7f70fd517700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 9 pgs degraded; 30 pgs peering; 14 pgs stale; 30 pgs stuck inactive; 39 pgs stuck unclean; 9 pgs undersized
2017-03-05 15:49:31.095828 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.954216s > max 0.05s
2017-03-05 15:49:31.095945 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.2 10.100.2.13:6789/0 clock skew 0.573805s > max 0.05s
2017-03-05 15:49:31.098340 7f70fd517700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:49:31.098417 7f70fd517700  0 log_channel(cluster) log [INF] : pgmap v32: 64 pgs: 9 active+undersized+degraded, 14 stale+active+clean, 30 peering, 11 active+clean; 0 bytes data, 101296 kB used, 27516 MB / 27614 MB avail
2017-03-05 15:49:31.098532 7f70fd517700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 15:49:31.098669 7f70fd517700  0 log_channel(cluster) log [INF] : osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.100521 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.957313s in the future, clocks not synchronized
2017-03-05 15:49:31.426101 7f71000a5700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:49:31.426258 7f71000a5700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:49:32.180968 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v33: 64 pgs: 9 active+undersized+degraded, 30 peering, 25 active+clean; 0 bytes data, 101888 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:49:34.204489 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v34: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:49:38.947092 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.382939s in the future, clocks not synchronized
2017-03-05 15:50:06.009455 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.379459s in the future, clocks not synchronized
2017-03-05 15:50:30.929660 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:50:31.093289 7f70fdd18700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 15:51:30.930318 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:51:32.453099 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v35: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:34.464602 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v36: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:35.476595 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:52:12.489543 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.380443s in the future, clocks not synchronized
2017-03-05 15:52:30.930927 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:53:30.931419 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:54:30.931968 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:54:31.094984 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.37869s > max 0.05s
2017-03-05 15:55:30.932447 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:56:30.932848 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:57:30.933248 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:58:30.933693 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:59:30.934186 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 16:00:00.000286 7f70fdd18700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 16:00:30.934772 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:01:30.935375 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:02:30.935987 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:03:30.936550 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:04:30.937020 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:05:30.937524 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:06:30.937981 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:07:30.938492 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:08:30.938930 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:09:30.939471 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:10:30.939928 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:11:30.940416 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:12:30.940849 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:13:30.941368 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:14:30.941857 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:14:43.887451 7f70fd517700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:14:43.887533 7f70fd517700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001804' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:15:30.942340 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:16:22.707681 7f70fd517700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:16:22.707705 7f70fd517700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001905' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:16:22.712935 7f70fd517700  1 mon.mon1@0(leader).log v70 check_sub sending message to client.? 10.100.0.72:0/1001905 with 1 entries (version 70)
2017-03-05 16:16:22.783657 7f70fed1a700  1 mon.mon1@0(leader).log v71 check_sub sending message to client.? 10.100.0.72:0/1001905 with 0 entries (version 71)
2017-03-05 16:16:30.942805 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:16:53.432490 7f70fbb12700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 16:16:53.432544 7f70fbb12700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 16:16:53.432603 7f70fbb12700  0 quorum service shutdown
2017-03-05 16:16:53.432608 7f70fbb12700  0 mon.mon1@0(shutdown).health(10) HealthMonitor::service_shutdown 1 services
2017-03-05 16:16:53.432611 7f70fbb12700  0 quorum service shutdown
2017-03-05 16:17:15.881457 7f458d4b27c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 834
2017-03-05 16:17:17.044452 7f458d4b27c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:17:17.045441 7f458d4b27c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:17:17.046329 7f458d4b27c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..37) refresh upgraded, format 0 -> 1
2017-03-05 16:17:17.046429 7f458d4b27c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 16:17:17.048043 7f458d4b27c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 16:17:17.048799 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048818 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048824 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048827 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.068028 7f458d4b27c0  1 mon.mon1@-1(probing).paxosservice(auth 1..12) refresh upgraded, format 0 -> 1
2017-03-05 16:17:17.094540 7f458d4b27c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 16:17:17.159060 7f4584977700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4fcc000 sd=20 :0 s=1 pgs=0 cs=0 l=0 c=0x4c27b80).fault
2017-03-05 16:17:17.159408 7f4584876700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x4fd1000 sd=21 :0 s=1 pgs=0 cs=0 l=0 c=0x4c27e40).fault
2017-03-05 16:17:17.599037 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:17.599271 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:18.412897 7f4583f74700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x4fea000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x4c28260).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:17:18.504445 7f458617a700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 16:17:18.504634 7f458617a700  1 mon.mon1@0(electing).elector(10) init, last seen epoch 10
2017-03-05 16:17:18.776753 7f4583f74700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4e00000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x4c28520).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:17:18.829261 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:18.829441 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:18.955958 7f458617a700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:17:18.960891 7f458617a700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:17:19.002642 7f458617a700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:17:19.002754 7f458617a700  0 log_channel(cluster) log [INF] : pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:19.002879 7f458617a700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 16:17:19.003118 7f458617a700  0 log_channel(cluster) log [INF] : osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 16:17:19.003622 7f458617a700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.0656535s > max 0.05s
2017-03-05 16:17:19.012168 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.110942s in the future, clocks not synchronized
2017-03-05 16:17:20.020280 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:20.020491 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:21.551875 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.552055 7f458617a700  0 log_channel(audit) log [INF] : from='client.24101 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.552541 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:21.795087 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.795142 7f458617a700  0 log_channel(audit) log [INF] : from='client.24104 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.795239 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:21.978179 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.978262 7f458617a700  0 log_channel(audit) log [INF] : from='client.24100 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.978573 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:23.617513 7f4587aff700  1 mon.mon1@0(leader).osd e23 e23: 3 osds: 2 up, 3 in
2017-03-05 16:17:23.621656 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e23: 3 osds: 2 up, 3 in
2017-03-05 16:17:23.627952 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v38: 64 pgs: 14 stale+active+clean, 50 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:23.637349 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.637405 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001118' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.637541 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:23.681313 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.681447 7f458617a700  0 log_channel(audit) log [INF] : from='client.24110 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.681603 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:23.811139 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.811213 7f458617a700  0 log_channel(audit) log [INF] : from='client.24103 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.811397 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:24.632636 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.108390s in the future, clocks not synchronized
2017-03-05 16:17:24.634337 7f4587aff700  1 mon.mon1@0(leader).osd e24 e24: 3 osds: 1 up, 3 in
2017-03-05 16:17:24.635553 7f4587aff700  0 log_channel(cluster) log [INF] : osd.2 10.100.1.13:6800/1135 boot
2017-03-05 16:17:24.640691 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e24: 3 osds: 1 up, 3 in
2017-03-05 16:17:24.646835 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v39: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:24.787835 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.787887 7f458617a700  0 log_channel(audit) log [INF] : from='client.24113 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.788039 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:24.971772 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.971826 7f458617a700  0 log_channel(audit) log [INF] : from='client.24116 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.971923 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:24.993184 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.993251 7f458617a700  0 log_channel(audit) log [INF] : from='client.24119 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.993454 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:25.643554 7f4587aff700  1 mon.mon1@0(leader).osd e25 e25: 3 osds: 3 up, 3 in
2017-03-05 16:17:25.644801 7f4587aff700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/1049 boot
2017-03-05 16:17:25.644920 7f4587aff700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/1078 boot
2017-03-05 16:17:25.647184 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e25: 3 osds: 3 up, 3 in
2017-03-05 16:17:25.651237 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v40: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:26.088882 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.088946 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001666' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.089117 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:26.252922 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.253018 7f458617a700  0 log_channel(audit) log [INF] : from='client.24122 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.253167 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:26.295393 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.295456 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1001660' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.295747 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:26.656410 7f4587aff700  1 mon.mon1@0(leader).osd e26 e26: 3 osds: 3 up, 3 in
2017-03-05 16:17:26.659194 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:17:26.667811 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v41: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:28.722712 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v42: 64 pgs: 50 stale+active+clean, 14 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:29.738142 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v43: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:51.746824 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.098338s in the future, clocks not synchronized
2017-03-05 16:18:17.070255 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:18:18.960770 7f458697b700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2; Monitor clock skew detected 
2017-03-05 16:19:17.070918 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:19:28.764199 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v44: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:29.775957 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:57.785440 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.059999s in the future, clocks not synchronized
2017-03-05 16:20:17.071476 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:21:17.072014 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:21:49.825497 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:21:49.825522 7f458617a700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001974' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:22:17.072591 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:23:17.073029 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:24:17.073561 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:25:17.074038 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:26:17.074544 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:27:17.075041 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:27:18.964062 7f458697b700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:28:17.075515 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:28:21.185293 7f4584775700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 16:28:21.185399 7f4584775700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 16:28:21.185471 7f4584775700  0 quorum service shutdown
2017-03-05 16:28:21.185474 7f4584775700  0 mon.mon1@0(shutdown).health(12) HealthMonitor::service_shutdown 1 services
2017-03-05 16:28:21.185478 7f4584775700  0 quorum service shutdown
2017-03-05 16:28:44.297456 7f7452aa77c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 825
2017-03-05 16:28:44.854394 7f7452aa77c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:28:44.855749 7f7452aa77c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:28:44.856928 7f7452aa77c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..45) refresh upgraded, format 0 -> 1
2017-03-05 16:28:44.856997 7f7452aa77c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 16:28:44.859291 7f7452aa77c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 16:28:44.860141 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860171 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860179 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860183 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.913945 7f7452aa77c0  1 mon.mon1@-1(probing).paxosservice(auth 1..14) refresh upgraded, format 0 -> 1
2017-03-05 16:28:44.937820 7f7452aa77c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 16:28:45.076048 7f7448ae7700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x3a61000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x37cb020).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:28:45.129650 7f744aeee700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 16:28:45.130033 7f744aeee700  1 mon.mon1@0(electing).elector(12) init, last seen epoch 12
2017-03-05 16:28:45.214727 7f744aeee700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:28:45.244804 7f744aeee700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:28:45.318525 7f744aeee700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:28:45.318646 7f744aeee700  0 log_channel(cluster) log [INF] : pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:28:45.318767 7f744aeee700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 16:28:45.319035 7f744aeee700  0 log_channel(cluster) log [INF] : osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:28:45.710376 7f744e47f700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:28:45.710620 7f744e47f700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:28:48.027491 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:48.027680 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34099 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.028017 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:50.016475 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:50.016533 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34103 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.016635 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:51.314771 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:51.314823 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34106 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.314930 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:52.533913 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:52.533977 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1001617' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:52.534112 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:29:44.917115 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:30:44.917995 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:31:44.918473 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:32:44.918927 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:33:44.919355 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:34:44.919878 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:35:44.920318 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:36:44.920903 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:37:44.921452 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:38:04.541408 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:04.541490 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34102 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.541589 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:06.248833 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:06.248909 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34109 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.249038 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:07.311949 7f744c8f3700  1 mon.mon1@0(leader).osd e27 e27: 3 osds: 2 up, 3 in
2017-03-05 16:38:07.315475 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e27: 3 osds: 2 up, 3 in
2017-03-05 16:38:07.329130 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v46: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:07.570878 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:07.570935 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001416' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:07.571068 7f744aeee700  0 mon.mon1@0(leader).osd e27 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:08.325971 7f744c8f3700  1 mon.mon1@0(leader).osd e28 e28: 3 osds: 3 up, 3 in
2017-03-05 16:38:08.327201 7f744c8f3700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/1092 boot
2017-03-05 16:38:08.328543 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e28: 3 osds: 3 up, 3 in
2017-03-05 16:38:08.331187 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v47: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:08.912927 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:08.912981 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34105 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.913117 7f744aeee700  0 mon.mon1@0(leader).osd e28 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:09.280035 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:09.280090 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34108 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.280196 7f744aeee700  0 mon.mon1@0(leader).osd e28 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:09.338404 7f744c8f3700  1 mon.mon1@0(leader).osd e29 e29: 3 osds: 3 up, 3 in
2017-03-05 16:38:09.340289 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e29: 3 osds: 3 up, 3 in
2017-03-05 16:38:09.344222 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v48: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:11.682559 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:11.682606 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1001155' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:11.682718 7f744aeee700  0 mon.mon1@0(leader).osd e29 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:12.747085 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v49: 64 pgs: 28 peering, 36 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:12.754297 7f744c8f3700  1 mon.mon1@0(leader).osd e30 e30: 3 osds: 2 up, 3 in
2017-03-05 16:38:12.756862 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e30: 3 osds: 2 up, 3 in
2017-03-05 16:38:12.762629 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v50: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:13.174165 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:13.174219 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34115 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.174373 7f744aeee700  0 mon.mon1@0(leader).osd e30 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:13.758454 7f744c8f3700  1 mon.mon1@0(leader).osd e31 e31: 3 osds: 3 up, 3 in
2017-03-05 16:38:13.759440 7f744c8f3700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/1031 boot
2017-03-05 16:38:13.760752 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e31: 3 osds: 3 up, 3 in
2017-03-05 16:38:13.764511 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v51: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:14.770674 7f744c8f3700  1 mon.mon1@0(leader).osd e32 e32: 3 osds: 3 up, 3 in
2017-03-05 16:38:14.772459 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e32: 3 osds: 3 up, 3 in
2017-03-05 16:38:14.776454 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v52: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:17.395005 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v53: 64 pgs: 41 peering, 23 active+clean; 0 bytes data, 102096 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:19.567219 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v54: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102364 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:44.922006 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:38:45.248089 7f744b6ef700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 30 pgs peering; 30 pgs stuck inactive; 30 pgs stuck unclean
2017-03-05 16:39:44.922500 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:40:19.605507 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v55: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:40:44.922977 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:41:44.923509 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:42:44.924059 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:43:44.924537 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:44:44.924979 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:45:44.925390 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:46:44.925822 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:47:44.926248 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:48:44.926734 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:49:44.927166 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:50:44.927613 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:51:44.927992 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:52:44.928358 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:53:44.928808 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:54:44.929234 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:55:44.929688 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:56:44.930171 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:57:44.930680 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:58:44.931172 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:58:50.056058 7f744b6ef700  0 log_channel(cluster) log [INF] : osd.2 marked down after no pg stats for 900.058900seconds
2017-03-05 16:58:50.056127 7f744b6ef700 -1 mon.mon1@0(leader).osd e32 no osd or pg stats from osd.2 since 2017-03-05 16:43:49.997129, 900.058900 seconds ago.  marking down
2017-03-05 16:58:50.063761 7f744c8f3700  1 mon.mon1@0(leader).osd e33 e33: 3 osds: 2 up, 3 in
2017-03-05 16:58:50.065987 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e33: 3 osds: 2 up, 3 in
2017-03-05 16:58:50.071303 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v56: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:51.076694 7f744c8f3700  1 mon.mon1@0(leader).osd e34 e34: 3 osds: 2 up, 3 in
2017-03-05 16:58:51.078777 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e34: 3 osds: 2 up, 3 in
2017-03-05 16:58:51.086914 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v57: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:55.822719 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v58: 64 pgs: 21 active+undersized+degraded, 6 stale+active+clean, 17 peering, 20 active+clean; 0 bytes data, 102372 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:58:56.837200 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v59: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
[root@mon1 ~]# vi /etc/ntp
ntp/      ntp.conf  
[root@mon1 ~]# vi /etc/ntp
ntp/      ntp.conf  
[root@mon1 ~]# vi /etc/ntp.conf 
[root@mon1 ~]# vi /etc/ntp/
crypto/       keys          step-tickers  
[root@mon1 ~]# vi /etc/ntp/
crypto/       keys          step-tickers  
[root@mon1 ~]# vi /etc/ntp.conf 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# cat /var/log/ceph/ceph.audit.log 
2017-03-05 15:19:36.677540 mon.1 10.100.2.12:6789/0 38 : audit [INF] from='client.? 10.100.2.12:0/1008928' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.680288 mon.0 10.100.2.11:6789/0 60 : audit [INF] from='client.4099 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.687102 mon.0 10.100.2.11:6789/0 61 : audit [INF] from='client.4099 :/0' entity='mon.' cmd='[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]': finished
2017-03-05 15:19:37.215851 mon.2 10.100.2.13:6789/0 27 : audit [INF] from='client.? 10.100.2.12:0/1008963' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.217753 mon.0 10.100.2.11:6789/0 62 : audit [INF] from='client.4100 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.238164 mon.0 10.100.2.11:6789/0 63 : audit [INF] from='client.4100 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]': finished
2017-03-05 15:19:37.539069 mon.1 10.100.2.12:6789/0 39 : audit [INF] from='client.? 10.100.2.13:0/1008929' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.541079 mon.0 10.100.2.11:6789/0 64 : audit [INF] from='client.4102 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.719385 mon.2 10.100.2.13:6789/0 28 : audit [INF] from='client.? 10.100.2.12:0/1008996' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.721040 mon.0 10.100.2.11:6789/0 65 : audit [INF] from='client.4103 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.745071 mon.0 10.100.2.11:6789/0 66 : audit [INF] from='client.4103 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]': finished
2017-03-05 15:19:37.945696 mon.2 10.100.2.13:6789/0 29 : audit [INF] from='client.? 10.100.2.13:0/1008964' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.947374 mon.0 10.100.2.11:6789/0 67 : audit [INF] from='client.4106 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:38.173084 mon.1 10.100.2.12:6789/0 40 : audit [INF] from='client.? 10.100.2.12:0/1009029' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.175433 mon.0 10.100.2.11:6789/0 68 : audit [INF] from='client.4105 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.198421 mon.0 10.100.2.11:6789/0 69 : audit [INF] from='client.4105 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]': finished
2017-03-05 15:19:38.377593 mon.1 10.100.2.12:6789/0 41 : audit [INF] from='client.? 10.100.2.13:0/1008998' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.379691 mon.0 10.100.2.11:6789/0 70 : audit [INF] from='client.4108 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.777147 mon.1 10.100.2.12:6789/0 42 : audit [INF] from='client.? 10.100.2.13:0/1009030' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.778896 mon.0 10.100.2.11:6789/0 71 : audit [INF] from='client.4111 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:39.448305 mon.1 10.100.2.12:6789/0 43 : audit [INF] from='client.? 10.100.2.11:0/1008970' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.449979 mon.0 10.100.2.11:6789/0 72 : audit [INF] from='client.4114 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.853054 mon.0 10.100.2.11:6789/0 73 : audit [INF] from='client.? 10.100.2.11:0/1009005' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:40.264364 mon.2 10.100.2.13:6789/0 30 : audit [INF] from='client.? 10.100.2.11:0/1009039' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.265910 mon.0 10.100.2.11:6789/0 74 : audit [INF] from='client.4109 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.670360 mon.2 10.100.2.13:6789/0 31 : audit [INF] from='client.? 10.100.2.11:0/1009071' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:40.671760 mon.0 10.100.2.11:6789/0 75 : audit [INF] from='client.4112 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:26:13.363578 mon.2 10.100.2.13:6789/0 34 : audit [INF] from='client.? 10.100.1.11:0/1008847' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.365838 mon.0 10.100.2.11:6789/0 79 : audit [INF] from='client.4115 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.373753 mon.0 10.100.2.11:6789/0 80 : audit [INF] from='client.4115 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]': finished
2017-03-05 15:26:14.501535 mon.2 10.100.2.13:6789/0 36 : audit [INF] from='client.? 10.100.1.11:0/1008938' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.503751 mon.0 10.100.2.11:6789/0 83 : audit [INF] from='client.4121 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.509119 mon.0 10.100.2.11:6789/0 84 : audit [INF] from='client.4121 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:15.792232 mon.0 10.100.2.11:6789/0 85 : audit [INF] from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 15:26:16.462114 mon.0 10.100.2.11:6789/0 86 : audit [INF] from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]': finished
2017-03-05 15:26:24.405756 mon.1 10.100.2.12:6789/0 46 : audit [INF] from='client.? 10.100.1.12:0/1008833' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.408008 mon.0 10.100.2.11:6789/0 95 : audit [INF] from='client.4120 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.988022 mon.0 10.100.2.11:6789/0 96 : audit [INF] from='client.4120 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]': finished
2017-03-05 15:26:26.058721 mon.1 10.100.2.12:6789/0 48 : audit [INF] from='client.? 10.100.1.12:0/1008923' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.061044 mon.0 10.100.2.11:6789/0 99 : audit [INF] from='client.4126 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.066846 mon.0 10.100.2.11:6789/0 100 : audit [INF] from='client.4126 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:27.511254 mon.2 10.100.2.13:6789/0 37 : audit [INF] from='client.? 10.100.1.12:0/1009091' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:27.513219 mon.0 10.100.2.11:6789/0 101 : audit [INF] from='client.4124 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:28.012613 mon.0 10.100.2.11:6789/0 102 : audit [INF] from='client.4124 :/0' entity='osd.1' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]': finished
2017-03-05 15:26:35.709195 mon.0 10.100.2.11:6789/0 112 : audit [INF] from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]: dispatch
2017-03-05 15:26:36.659122 mon.0 10.100.2.11:6789/0 114 : audit [INF] from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]': finished
2017-03-05 15:26:37.733812 mon.0 10.100.2.11:6789/0 118 : audit [INF] from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:37.739715 mon.0 10.100.2.11:6789/0 119 : audit [INF] from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:39.015598 mon.1 10.100.2.12:6789/0 49 : audit [INF] from='client.? 10.100.1.13:0/1009103' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.017825 mon.0 10.100.2.11:6789/0 120 : audit [INF] from='client.4129 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.693051 mon.0 10.100.2.11:6789/0 121 : audit [INF] from='client.4129 :/0' entity='osd.2' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]': finished
2017-03-05 16:17:21.549509 mon.2 10.100.2.13:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1000896' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.552062 mon.0 10.100.2.11:6789/0 16 : audit [INF] from='client.24101 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.793832 mon.2 10.100.2.13:6789/0 7 : audit [INF] from='client.? 10.100.1.13:0/1001061' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.795145 mon.0 10.100.2.11:6789/0 17 : audit [INF] from='client.24104 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.978268 mon.0 10.100.2.11:6789/0 18 : audit [INF] from='client.24100 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:22.087369 mon.1 10.100.2.12:6789/0 4 : audit [INF] from='client.? 10.100.1.11:0/1000920' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.637410 mon.0 10.100.2.11:6789/0 21 : audit [INF] from='client.? 10.100.1.12:0/1001118' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.680179 mon.2 10.100.2.13:6789/0 8 : audit [INF] from='client.? 10.100.1.13:0/1000892' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.681455 mon.0 10.100.2.11:6789/0 22 : audit [INF] from='client.24110 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.811218 mon.0 10.100.2.11:6789/0 23 : audit [INF] from='client.24103 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.919879 mon.1 10.100.2.12:6789/0 5 : audit [INF] from='client.? 10.100.1.11:0/1001122' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.786703 mon.2 10.100.2.13:6789/0 9 : audit [INF] from='client.? 10.100.1.12:0/1001444' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.787891 mon.0 10.100.2.11:6789/0 28 : audit [INF] from='client.24113 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.970634 mon.2 10.100.2.13:6789/0 10 : audit [INF] from='client.? 10.100.1.13:0/1001418' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.971829 mon.0 10.100.2.11:6789/0 29 : audit [INF] from='client.24116 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.992162 mon.2 10.100.2.13:6789/0 11 : audit [INF] from='client.? 10.100.1.11:0/1001438' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.993256 mon.0 10.100.2.11:6789/0 30 : audit [INF] from='client.24119 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.088951 mon.0 10.100.2.11:6789/0 35 : audit [INF] from='client.? 10.100.1.12:0/1001666' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.251556 mon.2 10.100.2.13:6789/0 12 : audit [INF] from='client.? 10.100.1.13:0/1001640' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.253025 mon.0 10.100.2.11:6789/0 36 : audit [INF] from='client.24122 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.295461 mon.0 10.100.2.11:6789/0 37 : audit [INF] from='client.? 10.100.1.11:0/1001660' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.025454 mon.1 10.100.2.12:6789/0 4 : audit [INF] from='client.? 10.100.1.13:0/1001084' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.027686 mon.0 10.100.2.11:6789/0 10 : audit [INF] from='client.34099 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.014671 mon.2 10.100.2.13:6789/0 4 : audit [INF] from='client.? 10.100.1.13:0/1000893' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.016537 mon.0 10.100.2.11:6789/0 11 : audit [INF] from='client.34103 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.313298 mon.2 10.100.2.13:6789/0 5 : audit [INF] from='client.? 10.100.1.13:0/1001417' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.314827 mon.0 10.100.2.11:6789/0 12 : audit [INF] from='client.34106 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:52.533982 mon.0 10.100.2.11:6789/0 13 : audit [INF] from='client.? 10.100.1.13:0/1001617' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.539484 mon.1 10.100.2.12:6789/0 5 : audit [INF] from='client.? 10.100.1.12:0/1000925' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.541498 mon.0 10.100.2.11:6789/0 14 : audit [INF] from='client.34102 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.247140 mon.2 10.100.2.13:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1001120' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.248913 mon.0 10.100.2.11:6789/0 15 : audit [INF] from='client.34109 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:07.570939 mon.0 10.100.2.11:6789/0 18 : audit [INF] from='client.? 10.100.1.12:0/1001416' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.910830 mon.1 10.100.2.12:6789/0 6 : audit [INF] from='client.? 10.100.1.12:0/1001622' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.912985 mon.0 10.100.2.11:6789/0 22 : audit [INF] from='client.34105 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.278050 mon.1 10.100.2.12:6789/0 7 : audit [INF] from='client.? 10.100.1.11:0/1000877' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.280094 mon.0 10.100.2.11:6789/0 23 : audit [INF] from='client.34108 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:11.682610 mon.0 10.100.2.11:6789/0 26 : audit [INF] from='client.? 10.100.1.11:0/1001155' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.172357 mon.2 10.100.2.13:6789/0 7 : audit [INF] from='client.? 10.100.1.11:0/1001508' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.174223 mon.0 10.100.2.11:6789/0 30 : audit [INF] from='client.34115 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# cat /var/log/ceph/ceph-mon.mon1.log 
2017-03-05 15:19:15.193363 7f11ef2ce7c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 8786
2017-03-05 15:19:15.365268 7f11ef2ce7c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:19:15.366316 7f11ef2ce7c0  1 mon.mon1@-1(probing) e0 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:19:15.366439 7f11ef2ce7c0  1 mon.mon1@-1(probing) e0  initial_members mon1,mon2,mon3, filtering seed monmap
2017-03-05 15:19:15.369379 7f11ef2ce7c0  0 mon.mon1@-1(probing) e0  my rank is now 0 (was -1)
2017-03-05 15:19:15.379863 7f11e6915700  0 -- 10.100.2.11:6789/0 >> 0.0.0.0:0/2 pipe(0x42e1000 sd=23 :0 s=1 pgs=0 cs=0 l=0 c=0x417bce0).fault
2017-03-05 15:19:15.380295 7f11e6713700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42eb000 sd=20 :0 s=1 pgs=0 cs=0 l=0 c=0x417c260).fault
2017-03-05 15:19:15.390886 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x42e6000 sd=22 :0 s=1 pgs=0 cs=0 l=0 c=0x417bfa0).fault
2017-03-05 15:19:15.391354 7f11ef2bc700  0 -- 10.100.2.11:6789/0 >> 0.0.0.0:0/1 pipe(0x42dc000 sd=21 :0 s=1 pgs=0 cs=0 l=0 c=0x417ba20).fault
2017-03-05 15:19:15.644907 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:15.645105 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:16.853956 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:16.854096 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.159597 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.159747 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.221830 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.221993 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:18.462388 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:18.462634 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:19.396688 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:19.396818 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:20.583908 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:20.584042 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:20.626451 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4320000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c680).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 15:19:20.678446 7f11e8118700  1 mon.mon1@0(probing) e0  adding peer 10.100.2.12:6789/0 to list of hints
2017-03-05 15:19:20.678518 7f11e8118700  1 mon.mon1@0(probing) e0  learned initial mon mon2 addr 10.100.2.12:6789/0
2017-03-05 15:19:20.679020 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:20.679151 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:20.684199 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x42e6000 sd=22 :36357 s=2 pgs=8 cs=1 l=0 c=0x417bfa0).fault, initiating reconnect
2017-03-05 15:19:20.712291 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x433a000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c940).accept connect_seq 0 vs existing 2 state connecting
2017-03-05 15:19:20.712360 7f11e5e11700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x433a000 sd=24 :6789 s=0 pgs=0 cs=0 l=0 c=0x417c940).accept peer reset, then tried to connect to us, replacing
2017-03-05 15:19:21.770592 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:21.770737 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:22.960185 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:22.960377 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:24.151753 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:24.151894 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:25.333645 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:25.333818 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:25.682039 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:25.682136 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:25.844137 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x433f000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cc00).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 15:19:25.896087 7f11e8118700  1 mon.mon1@0(electing) e0  adding peer 10.100.2.13:6789/0 to list of hints
2017-03-05 15:19:25.896131 7f11e8118700  1 mon.mon1@0(electing) e0  learned initial mon mon3 addr 10.100.2.13:6789/0
2017-03-05 15:19:25.896795 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:25.896885 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:25.907650 7f11e5d10700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42eb000 sd=20 :50940 s=2 pgs=7 cs=1 l=0 c=0x417c260).fault, initiating reconnect
2017-03-05 15:19:25.927681 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cd60).accept connect_seq 0 vs existing 2 state connecting
2017-03-05 15:19:25.927709 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=0 pgs=0 cs=0 l=0 c=0x417cd60).accept peer reset, then tried to connect to us, replacing
2017-03-05 15:19:26.517981 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:26.518116 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:27.698725 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:27.698855 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:28.874882 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:28.875021 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:29.886179 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:29.886406 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:30.067277 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:30.067404 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:30.900094 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:30.900189 7f11e8118700  1 mon.mon1@0(electing).elector(1) init, last seen epoch 1
2017-03-05 15:19:31.255849 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:31.255983 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:32.436712 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:32.436930 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:33.618838 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:33.618962 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:34.782511 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:34.782680 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:35.094800 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:35.094940 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:35.913846 7f11e8919700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,2
2017-03-05 15:19:35.924230 7f11e8919700  1 mon.mon1@0(leader) e0 apply_quorum_to_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code}
2017-03-05 15:19:35.947782 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:35.947850 7f11e8118700  1 mon.mon1@0(electing).elector(2) init, last seen epoch 2
2017-03-05 15:19:35.976994 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:35.980006 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:35.980088 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:19:36.049002 7f11e991b700  1 mon.mon1@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.049034 7f11e991b700  1 mon.mon1@0(leader).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 15:19:36.061755 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.075818 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:19:36.075920 7f11e8118700  1 mon.mon1@0(electing).elector(4) init, last seen epoch 4
2017-03-05 15:19:36.123325 7f11e8118700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:36.136861 7f11e8118700  0 log_channel(cluster) log [INF] : HEALTH_ERR; no osds
2017-03-05 15:19:36.187014 7f11e8118700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:19:36.236729 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 0..0) refresh upgraded, format 1 -> 0
2017-03-05 15:19:36.236906 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v1: 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:19:36.301056 7f11e991b700  0 mon.mon1@0(leader).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 15:19:36.301332 7f11e991b700  1 mon.mon1@0(leader).osd e1 e1: 0 osds: 0 up, 0 in
2017-03-05 15:19:36.303472 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303479 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303484 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.303488 7f11e991b700  0 mon.mon1@0(leader).osd e1 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:19:36.304186 7f11e991b700  1 mon.mon1@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 1
2017-03-05 15:19:36.305385 7f11e991b700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 15:19:36.312205 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e1: 0 osds: 0 up, 0 in
2017-03-05 15:19:36.377879 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v2: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:19:36.680185 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:36.680282 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4099 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:36.687097 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4099 :/0' entity='mon.' cmd='[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]': finished
2017-03-05 15:19:37.217716 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:37.217749 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4100 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:37.238159 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4100 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]': finished
2017-03-05 15:19:37.541037 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:37.541074 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4102 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:37.721005 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:37.721036 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4103 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:37.745015 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4103 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]': finished
2017-03-05 15:19:37.947338 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:37.947370 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4106 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:38.175373 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:38.175422 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4105 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:38.198416 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4105 :/0' entity='client.admin' cmd='[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]': finished
2017-03-05 15:19:38.379651 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:38.379687 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4108 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:38.778841 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:38.778887 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4111 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:39.449922 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]} v 0) v1
2017-03-05 15:19:39.449970 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4114 :/0' entity='mon.' cmd=[{"prefix": "auth get-or-create", "entity": "client.admin", "caps": ["mon", "allow *", "osd", "allow *", "mds", "allow"]}]: dispatch
2017-03-05 15:19:39.853007 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]} v 0) v1
2017-03-05 15:19:39.853046 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.2.11:0/1009005' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-osd", "caps": ["mon", "allow profile bootstrap-osd"]}]: dispatch
2017-03-05 15:19:40.265875 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]} v 0) v1
2017-03-05 15:19:40.265905 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4109 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-rgw", "caps": ["mon", "allow profile bootstrap-rgw"]}]: dispatch
2017-03-05 15:19:40.671722 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]} v 0) v1
2017-03-05 15:19:40.671755 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4112 :/0' entity='client.admin' cmd=[{"prefix": "auth get-or-create", "entity": "client.bootstrap-mds", "caps": ["mon", "allow profile bootstrap-mds"]}]: dispatch
2017-03-05 15:19:45.343646 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:19:45.343777 7f11eaca6700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:20:15.367428 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1687 MB, avail 4446 MB
2017-03-05 15:20:36.137137 7f11e8919700  0 log_channel(cluster) log [INF] : HEALTH_ERR; 64 pgs stuck inactive; 64 pgs stuck unclean; no osds
2017-03-05 15:21:15.368497 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 71% total 6134 MB, used 1741 MB, avail 4392 MB
2017-03-05 15:22:15.369366 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:23:15.370106 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:24:15.370872 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:25:15.371498 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:26:13.365679 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"} v 0) v1
2017-03-05 15:26:13.365832 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4115 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]: dispatch
2017-03-05 15:26:13.372625 7f11e991b700  1 mon.mon1@0(leader).osd e2 e2: 1 osds: 0 up, 0 in
2017-03-05 15:26:13.373741 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4115 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "4293040e-1ae5-4d63-abee-bd8711896bf4"}]': finished
2017-03-05 15:26:13.375167 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e2: 1 osds: 0 up, 0 in
2017-03-05 15:26:13.378881 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v3: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:14.503704 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:14.503746 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4121 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:14.509114 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4121 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.0", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:15.372087 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:26:15.792154 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 15:26:15.792226 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 15:26:15.792434 7f11e8118700  0 mon.mon1@0(leader).osd e2 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 15:26:16.460129 7f11e991b700  1 mon.mon1@0(leader).osd e3 e3: 1 osds: 0 up, 0 in
2017-03-05 15:26:16.462107 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1009106' entity='osd.0' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]': finished
2017-03-05 15:26:16.463453 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e3: 1 osds: 0 up, 0 in
2017-03-05 15:26:16.466949 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v4: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:17.477868 7f11e991b700  1 mon.mon1@0(leader).osd e4 e4: 1 osds: 1 up, 1 in
2017-03-05 15:26:17.479796 7f11e991b700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/9152 boot
2017-03-05 15:26:17.481317 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e4: 1 osds: 1 up, 1 in
2017-03-05 15:26:17.485166 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v5: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:18.495168 7f11e991b700  1 mon.mon1@0(leader).osd e5 e5: 1 osds: 1 up, 1 in
2017-03-05 15:26:18.498435 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e5: 1 osds: 1 up, 1 in
2017-03-05 15:26:18.503438 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v6: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:22.964000 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v7: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:24.407956 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"} v 0) v1
2017-03-05 15:26:24.408003 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4120 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]: dispatch
2017-03-05 15:26:24.986891 7f11e991b700  1 mon.mon1@0(leader).osd e6 e6: 2 osds: 1 up, 1 in
2017-03-05 15:26:24.988017 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4120 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "9edf911f-29b4-48f2-9cd5-1a7a0d1d4a3c"}]': finished
2017-03-05 15:26:24.990543 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e6: 2 osds: 1 up, 1 in
2017-03-05 15:26:24.994275 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v8: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:26.060982 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:26.061039 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4126 :/0' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:26.066841 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4126 :/0' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.1", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:27.513149 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 15:26:27.513213 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4124 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 15:26:27.513380 7f11e8118700  0 mon.mon1@0(leader).osd e6 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 15:26:28.011015 7f11e991b700  1 mon.mon1@0(leader).osd e7 e7: 2 osds: 1 up, 1 in
2017-03-05 15:26:28.012607 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4124 :/0' entity='osd.1' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]': finished
2017-03-05 15:26:28.015177 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e7: 2 osds: 1 up, 1 in
2017-03-05 15:26:28.021427 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v9: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:29.037826 7f11e991b700  1 mon.mon1@0(leader).osd e8 e8: 2 osds: 2 up, 2 in
2017-03-05 15:26:29.038850 7f11e991b700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/9137 boot
2017-03-05 15:26:29.040541 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e8: 2 osds: 2 up, 2 in
2017-03-05 15:26:29.049311 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v10: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:30.054978 7f11e991b700  1 mon.mon1@0(leader).osd e9 e9: 2 osds: 2 up, 2 in
2017-03-05 15:26:30.057323 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e9: 2 osds: 2 up, 2 in
2017-03-05 15:26:30.063746 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v11: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:34.635212 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v12: 64 pgs: 31 active+undersized+degraded, 33 active+clean; 0 bytes data, 68064 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:35.651446 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v13: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:35.709141 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"} v 0) v1
2017-03-05 15:26:35.709190 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd=[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]: dispatch
2017-03-05 15:26:36.140343 7f11e8919700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 15:26:36.657756 7f11e991b700  1 mon.mon1@0(leader).osd e10 e10: 3 osds: 2 up, 2 in
2017-03-05 15:26:36.659117 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008845' entity='client.bootstrap-osd' cmd='[{"prefix": "osd create", "uuid": "2c54f9e2-dfe7-4cf7-94f3-8eb6c5cb8213"}]': finished
2017-03-05 15:26:36.660933 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e10: 3 osds: 2 up, 2 in
2017-03-05 15:26:36.667960 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v14: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:37.061120 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "mon getmap"} v 0) v1
2017-03-05 15:26:37.061162 7f11e8118700  0 log_channel(audit) log [DBG] : from='client.? 10.100.1.13:0/1008877' entity='client.bootstrap-osd' cmd=[{"prefix": "mon getmap"}]: dispatch
2017-03-05 15:26:37.733762 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]} v 0) v1
2017-03-05 15:26:37.733807 7f11e8118700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd=[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]: dispatch
2017-03-05 15:26:37.739710 7f11e991b700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1008935' entity='client.bootstrap-osd' cmd='[{"prefix": "auth add", "entity": "osd.2", "caps": ["osd", "allow *", "mon", "allow profile osd"]}]': finished
2017-03-05 15:26:39.017749 7f11e8118700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 15:26:39.017816 7f11e8118700  0 log_channel(audit) log [INF] : from='client.4129 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 15:26:39.017946 7f11e8118700  0 mon.mon1@0(leader).osd e10 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 15:26:39.691814 7f11e991b700  1 mon.mon1@0(leader).osd e11 e11: 3 osds: 2 up, 2 in
2017-03-05 15:26:39.693046 7f11e991b700  0 log_channel(audit) log [INF] : from='client.4129 :/0' entity='osd.2' cmd='[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]': finished
2017-03-05 15:26:39.695079 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e11: 3 osds: 2 up, 2 in
2017-03-05 15:26:39.701775 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v15: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:40.699678 7f11e991b700  1 mon.mon1@0(leader).osd e12 e12: 3 osds: 3 up, 3 in
2017-03-05 15:26:40.700949 7f11e991b700  0 log_channel(cluster) log [INF] : osd.2 10.100.1.13:6800/9177 boot
2017-03-05 15:26:40.702990 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e12: 3 osds: 3 up, 3 in
2017-03-05 15:26:40.712102 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v16: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:41.718656 7f11e991b700  1 mon.mon1@0(leader).osd e13 e13: 3 osds: 3 up, 3 in
2017-03-05 15:26:41.721312 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e13: 3 osds: 3 up, 3 in
2017-03-05 15:26:41.726744 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v17: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:45.966382 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v18: 64 pgs: 64 active+clean; 0 bytes data, 68500 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:46.983972 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v19: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:27:15.372810 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:28:15.373583 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:28:46.011587 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v20: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:28:47.017419 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v21: 64 pgs: 64 active+clean; 0 bytes data, 102032 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:29:15.374399 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:30:15.375045 7f11e8919700  0 mon.mon1@0(leader).data_health(6) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 15:31:01.556835 7f11e8118700  0 log_channel(cluster) log [INF] : osd.1 marked itself down
2017-03-05 15:31:01.614633 7f11e991b700  1 mon.mon1@0(leader).osd e14 e14: 3 osds: 2 up, 3 in
2017-03-05 15:31:01.620221 7f11e991b700  0 log_channel(cluster) log [INF] : osdmap e14: 3 osds: 2 up, 3 in
2017-03-05 15:31:01.643642 7f11e991b700  0 log_channel(cluster) log [INF] : pgmap v22: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 102032 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:31:01.740238 7f11e6814700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x42e6000 sd=23 :6789 s=2 pgs=10 cs=1 l=0 c=0x417c260).fault with nothing to send, going to standby
2017-03-05 15:31:01.946374 7f11e8118700  0 log_channel(cluster) log [INF] : osd.0 marked itself down
2017-03-05 15:31:02.279173 7f11e6612700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 15:31:02.279231 7f11e6612700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 15:31:02.279654 7f11e6612700  0 quorum service shutdown
2017-03-05 15:31:02.279658 7f11e6612700  0 mon.mon1@0(shutdown).health(6) HealthMonitor::service_shutdown 1 services
2017-03-05 15:31:02.279663 7f11e6612700  0 quorum service shutdown
2017-03-05 15:49:29.981794 7f71046cd7c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 837
2017-03-05 15:49:30.922367 7f71046cd7c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:49:30.923368 7f71046cd7c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:49:30.924134 7f71046cd7c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..22) refresh upgraded, format 0 -> 1
2017-03-05 15:49:30.924180 7f71046cd7c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 15:49:30.925195 7f71046cd7c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 15:49:30.925767 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925787 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925791 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.925794 7f71046cd7c0  0 mon.mon1@-1(probing).osd e14 crush map has features 1107558400, adjusting msgr requires
2017-03-05 15:49:30.928566 7f71046cd7c0  1 mon.mon1@-1(probing).paxosservice(auth 1..9) refresh upgraded, format 0 -> 1
2017-03-05 15:49:30.931241 7f71046cd7c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 15:49:31.079312 7f70fd517700  1 mon.mon1@0(synchronizing).osd e15 e15: 3 osds: 1 up, 3 in
2017-03-05 15:49:31.079480 7f70fd517700  1 mon.mon1@0(synchronizing).osd e16 e16: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079532 7f70fd517700  1 mon.mon1@0(synchronizing).osd e17 e17: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079570 7f70fd517700  1 mon.mon1@0(synchronizing).osd e18 e18: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079603 7f70fd517700  1 mon.mon1@0(synchronizing).osd e19 e19: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079728 7f70fd517700  1 mon.mon1@0(synchronizing).osd e20 e20: 3 osds: 2 up, 3 in
2017-03-05 15:49:31.079777 7f70fd517700  1 mon.mon1@0(synchronizing).osd e21 e21: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.079817 7f70fd517700  1 mon.mon1@0(synchronizing).osd e22 e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.083702 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:49:31.083820 7f70fd517700  1 mon.mon1@0(electing).elector(6) init, last seen epoch 6
2017-03-05 15:49:31.088928 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 15:49:31.089038 7f70fd517700  1 mon.mon1@0(electing).elector(9) init, last seen epoch 9
2017-03-05 15:49:31.090134 7f70fd517700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:49:31.093502 7f70fd517700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 9 pgs degraded; 30 pgs peering; 14 pgs stale; 30 pgs stuck inactive; 39 pgs stuck unclean; 9 pgs undersized
2017-03-05 15:49:31.095828 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.954216s > max 0.05s
2017-03-05 15:49:31.095945 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.2 10.100.2.13:6789/0 clock skew 0.573805s > max 0.05s
2017-03-05 15:49:31.098340 7f70fd517700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:49:31.098417 7f70fd517700  0 log_channel(cluster) log [INF] : pgmap v32: 64 pgs: 9 active+undersized+degraded, 14 stale+active+clean, 30 peering, 11 active+clean; 0 bytes data, 101296 kB used, 27516 MB / 27614 MB avail
2017-03-05 15:49:31.098532 7f70fd517700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 15:49:31.098669 7f70fd517700  0 log_channel(cluster) log [INF] : osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.100521 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.957313s in the future, clocks not synchronized
2017-03-05 15:49:31.426101 7f71000a5700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 15:49:31.426258 7f71000a5700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 15:49:32.180968 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v33: 64 pgs: 9 active+undersized+degraded, 30 peering, 25 active+clean; 0 bytes data, 101888 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:49:34.204489 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v34: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:49:38.947092 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.382939s in the future, clocks not synchronized
2017-03-05 15:50:06.009455 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.379459s in the future, clocks not synchronized
2017-03-05 15:50:30.929660 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:50:31.093289 7f70fdd18700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 15:51:30.930318 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:51:32.453099 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v35: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:34.464602 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v36: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:35.476595 7f70fed1a700  0 log_channel(cluster) log [INF] : pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:52:12.489543 7f70fd517700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.380443s in the future, clocks not synchronized
2017-03-05 15:52:30.930927 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:53:30.931419 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:54:30.931968 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:54:31.094984 7f70fd517700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.37869s > max 0.05s
2017-03-05 15:55:30.932447 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:56:30.932848 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:57:30.933248 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:58:30.933693 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 15:59:30.934186 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1708 MB, avail 4425 MB
2017-03-05 16:00:00.000286 7f70fdd18700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 16:00:30.934772 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:01:30.935375 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:02:30.935987 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:03:30.936550 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:04:30.937020 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:05:30.937524 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:06:30.937981 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:07:30.938492 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:08:30.938930 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:09:30.939471 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:10:30.939928 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:11:30.940416 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:12:30.940849 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:13:30.941368 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:14:30.941857 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:14:43.887451 7f70fd517700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:14:43.887533 7f70fd517700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001804' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:15:30.942340 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:16:22.707681 7f70fd517700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:16:22.707705 7f70fd517700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001905' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:16:22.712935 7f70fd517700  1 mon.mon1@0(leader).log v70 check_sub sending message to client.? 10.100.0.72:0/1001905 with 1 entries (version 70)
2017-03-05 16:16:22.783657 7f70fed1a700  1 mon.mon1@0(leader).log v71 check_sub sending message to client.? 10.100.0.72:0/1001905 with 0 entries (version 71)
2017-03-05 16:16:30.942805 7f70fdd18700  0 mon.mon1@0(leader).data_health(10) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:16:53.432490 7f70fbb12700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 16:16:53.432544 7f70fbb12700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 16:16:53.432603 7f70fbb12700  0 quorum service shutdown
2017-03-05 16:16:53.432608 7f70fbb12700  0 mon.mon1@0(shutdown).health(10) HealthMonitor::service_shutdown 1 services
2017-03-05 16:16:53.432611 7f70fbb12700  0 quorum service shutdown
2017-03-05 16:17:15.881457 7f458d4b27c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 834
2017-03-05 16:17:17.044452 7f458d4b27c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:17:17.045441 7f458d4b27c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:17:17.046329 7f458d4b27c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..37) refresh upgraded, format 0 -> 1
2017-03-05 16:17:17.046429 7f458d4b27c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 16:17:17.048043 7f458d4b27c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 16:17:17.048799 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048818 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048824 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.048827 7f458d4b27c0  0 mon.mon1@-1(probing).osd e22 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:17:17.068028 7f458d4b27c0  1 mon.mon1@-1(probing).paxosservice(auth 1..12) refresh upgraded, format 0 -> 1
2017-03-05 16:17:17.094540 7f458d4b27c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 16:17:17.159060 7f4584977700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4fcc000 sd=20 :0 s=1 pgs=0 cs=0 l=0 c=0x4c27b80).fault
2017-03-05 16:17:17.159408 7f4584876700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x4fd1000 sd=21 :0 s=1 pgs=0 cs=0 l=0 c=0x4c27e40).fault
2017-03-05 16:17:17.599037 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:17.599271 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:18.412897 7f4583f74700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x4fea000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x4c28260).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:17:18.504445 7f458617a700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 16:17:18.504634 7f458617a700  1 mon.mon1@0(electing).elector(10) init, last seen epoch 10
2017-03-05 16:17:18.776753 7f4583f74700  0 -- 10.100.2.11:6789/0 >> 10.100.2.12:6789/0 pipe(0x4e00000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x4c28520).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:17:18.829261 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:18.829441 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:18.955958 7f458617a700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:17:18.960891 7f458617a700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:17:19.002642 7f458617a700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:17:19.002754 7f458617a700  0 log_channel(cluster) log [INF] : pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:19.002879 7f458617a700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 16:17:19.003118 7f458617a700  0 log_channel(cluster) log [INF] : osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 16:17:19.003622 7f458617a700  0 log_channel(cluster) log [WRN] : mon.1 10.100.2.12:6789/0 clock skew 0.0656535s > max 0.05s
2017-03-05 16:17:19.012168 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.110942s in the future, clocks not synchronized
2017-03-05 16:17:20.020280 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:17:20.020491 7f4588e8a700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:17:21.551875 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.552055 7f458617a700  0 log_channel(audit) log [INF] : from='client.24101 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.552541 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:21.795087 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.795142 7f458617a700  0 log_channel(audit) log [INF] : from='client.24104 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.795239 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:21.978179 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:21.978262 7f458617a700  0 log_channel(audit) log [INF] : from='client.24100 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:21.978573 7f458617a700  0 mon.mon1@0(leader).osd e22 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:23.617513 7f4587aff700  1 mon.mon1@0(leader).osd e23 e23: 3 osds: 2 up, 3 in
2017-03-05 16:17:23.621656 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e23: 3 osds: 2 up, 3 in
2017-03-05 16:17:23.627952 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v38: 64 pgs: 14 stale+active+clean, 50 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:23.637349 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.637405 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001118' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.637541 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:23.681313 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.681447 7f458617a700  0 log_channel(audit) log [INF] : from='client.24110 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.681603 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:23.811139 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:23.811213 7f458617a700  0 log_channel(audit) log [INF] : from='client.24103 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:23.811397 7f458617a700  0 mon.mon1@0(leader).osd e23 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:24.632636 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.108390s in the future, clocks not synchronized
2017-03-05 16:17:24.634337 7f4587aff700  1 mon.mon1@0(leader).osd e24 e24: 3 osds: 1 up, 3 in
2017-03-05 16:17:24.635553 7f4587aff700  0 log_channel(cluster) log [INF] : osd.2 10.100.1.13:6800/1135 boot
2017-03-05 16:17:24.640691 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e24: 3 osds: 1 up, 3 in
2017-03-05 16:17:24.646835 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v39: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:24.787835 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.787887 7f458617a700  0 log_channel(audit) log [INF] : from='client.24113 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.788039 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:24.971772 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.971826 7f458617a700  0 log_channel(audit) log [INF] : from='client.24116 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.971923 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:24.993184 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:24.993251 7f458617a700  0 log_channel(audit) log [INF] : from='client.24119 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:24.993454 7f458617a700  0 mon.mon1@0(leader).osd e24 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:25.643554 7f4587aff700  1 mon.mon1@0(leader).osd e25 e25: 3 osds: 3 up, 3 in
2017-03-05 16:17:25.644801 7f4587aff700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/1049 boot
2017-03-05 16:17:25.644920 7f4587aff700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/1078 boot
2017-03-05 16:17:25.647184 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e25: 3 osds: 3 up, 3 in
2017-03-05 16:17:25.651237 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v40: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:26.088882 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.088946 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001666' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.089117 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:17:26.252922 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.253018 7f458617a700  0 log_channel(audit) log [INF] : from='client.24122 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.253167 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:17:26.295393 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:17:26.295456 7f458617a700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1001660' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:17:26.295747 7f458617a700  0 mon.mon1@0(leader).osd e25 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:17:26.656410 7f4587aff700  1 mon.mon1@0(leader).osd e26 e26: 3 osds: 3 up, 3 in
2017-03-05 16:17:26.659194 7f4587aff700  0 log_channel(cluster) log [INF] : osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:17:26.667811 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v41: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:28.722712 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v42: 64 pgs: 50 stale+active+clean, 14 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:29.738142 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v43: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:51.746824 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.098338s in the future, clocks not synchronized
2017-03-05 16:18:17.070255 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:18:18.960770 7f458697b700  0 log_channel(cluster) log [INF] : HEALTH_WARN; clock skew detected on mon.mon2; Monitor clock skew detected 
2017-03-05 16:19:17.070918 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1709 MB, avail 4424 MB
2017-03-05 16:19:28.764199 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v44: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:29.775957 7f4587aff700  0 log_channel(cluster) log [INF] : pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:57.785440 7f458617a700  0 log_channel(cluster) log [WRN] : message from mon.1 was stamped 0.059999s in the future, clocks not synchronized
2017-03-05 16:20:17.071476 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:21:17.072014 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:21:49.825497 7f458617a700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "status"} v 0) v1
2017-03-05 16:21:49.825522 7f458617a700  0 log_channel(audit) log [DBG] : from='client.? 10.100.0.72:0/1001974' entity='client.admin' cmd=[{"prefix": "status"}]: dispatch
2017-03-05 16:22:17.072591 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:23:17.073029 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:24:17.073561 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:25:17.074038 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:26:17.074544 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:27:17.075041 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:27:18.964062 7f458697b700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:28:17.075515 7f458697b700  0 mon.mon1@0(leader).data_health(12) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:28:21.185293 7f4584775700 -1 mon.mon1@0(leader) e1 *** Got Signal Terminated ***
2017-03-05 16:28:21.185399 7f4584775700  1 mon.mon1@0(leader) e1 shutdown
2017-03-05 16:28:21.185471 7f4584775700  0 quorum service shutdown
2017-03-05 16:28:21.185474 7f4584775700  0 mon.mon1@0(shutdown).health(12) HealthMonitor::service_shutdown 1 services
2017-03-05 16:28:21.185478 7f4584775700  0 quorum service shutdown
2017-03-05 16:28:44.297456 7f7452aa77c0  0 ceph version 0.94.3 (95cefea9fd9ab740263bf8bb4796fd864d9afe2b), process ceph-mon, pid 825
2017-03-05 16:28:44.854394 7f7452aa77c0  0 starting mon.mon1 rank 0 at 10.100.2.11:6789/0 mon_data /var/lib/ceph/mon/ceph-mon1 fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:28:44.855749 7f7452aa77c0  1 mon.mon1@-1(probing) e1 preinit fsid 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 16:28:44.856928 7f7452aa77c0  1 mon.mon1@-1(probing).paxosservice(pgmap 1..45) refresh upgraded, format 0 -> 1
2017-03-05 16:28:44.856997 7f7452aa77c0  1 mon.mon1@-1(probing).pg v0 on_upgrade discarding in-core PGMap
2017-03-05 16:28:44.859291 7f7452aa77c0  0 mon.mon1@-1(probing).mds e1 print_map
epoch	1
flags	0
created	0.000000
modified	2017-03-05 15:19:36.192206
tableserver	0
root	0
session_timeout	0
session_autoclose	0
max_file_size	0
last_failure	0
last_failure_osd_epoch	0
compat	compat={},rocompat={},incompat={}
max_mds	0
in	
up	{}
failed	
stopped	
data_pools	
metadata_pool	0
inline_data	disabled

2017-03-05 16:28:44.860141 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860171 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860179 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.860183 7f7452aa77c0  0 mon.mon1@-1(probing).osd e26 crush map has features 1107558400, adjusting msgr requires
2017-03-05 16:28:44.913945 7f7452aa77c0  1 mon.mon1@-1(probing).paxosservice(auth 1..14) refresh upgraded, format 0 -> 1
2017-03-05 16:28:44.937820 7f7452aa77c0  0 mon.mon1@-1(probing) e1  my rank is now 0 (was -1)
2017-03-05 16:28:45.076048 7f7448ae7700  0 -- 10.100.2.11:6789/0 >> 10.100.2.13:6789/0 pipe(0x3a61000 sd=22 :6789 s=0 pgs=0 cs=0 l=0 c=0x37cb020).accept connect_seq 0 vs existing 0 state connecting
2017-03-05 16:28:45.129650 7f744aeee700  0 log_channel(cluster) log [INF] : mon.mon1 calling new monitor election
2017-03-05 16:28:45.130033 7f744aeee700  1 mon.mon1@0(electing).elector(12) init, last seen epoch 12
2017-03-05 16:28:45.214727 7f744aeee700  0 log_channel(cluster) log [INF] : mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:28:45.244804 7f744aeee700  0 log_channel(cluster) log [INF] : HEALTH_OK
2017-03-05 16:28:45.318525 7f744aeee700  0 log_channel(cluster) log [INF] : monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:28:45.318646 7f744aeee700  0 log_channel(cluster) log [INF] : pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:28:45.318767 7f744aeee700  0 log_channel(cluster) log [INF] : mdsmap e1: 0/0/0 up
2017-03-05 16:28:45.319035 7f744aeee700  0 log_channel(cluster) log [INF] : osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:28:45.710376 7f744e47f700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd='mon_status' args=[]: dispatch
2017-03-05 16:28:45.710620 7f744e47f700  0 log_channel(audit) log [DBG] : from='admin socket' entity='admin socket' cmd=mon_status args=[]: finished
2017-03-05 16:28:48.027491 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:48.027680 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34099 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:48.028017 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:50.016475 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:50.016533 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34103 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:50.016635 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:51.314771 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:51.314823 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34106 :/0' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:51.314930 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:28:52.533913 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01} v 0) v1
2017-03-05 16:28:52.533977 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.13:0/1001617' entity='osd.2' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node3", "root=default"], "id": 2, "weight": 0.01}]: dispatch
2017-03-05 16:28:52.534112 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.2' initial_weight 0.01 at location {host=node3,root=default}
2017-03-05 16:29:44.917115 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:30:44.917995 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:31:44.918473 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:32:44.918927 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:33:44.919355 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:34:44.919878 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:35:44.920318 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:36:44.920903 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:37:44.921452 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:38:04.541408 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:04.541490 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34102 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:04.541589 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:06.248833 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:06.248909 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34109 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:06.249038 7f744aeee700  0 mon.mon1@0(leader).osd e26 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:07.311949 7f744c8f3700  1 mon.mon1@0(leader).osd e27 e27: 3 osds: 2 up, 3 in
2017-03-05 16:38:07.315475 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e27: 3 osds: 2 up, 3 in
2017-03-05 16:38:07.329130 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v46: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:07.570878 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:07.570935 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.12:0/1001416' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:07.571068 7f744aeee700  0 mon.mon1@0(leader).osd e27 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:08.325971 7f744c8f3700  1 mon.mon1@0(leader).osd e28 e28: 3 osds: 3 up, 3 in
2017-03-05 16:38:08.327201 7f744c8f3700  0 log_channel(cluster) log [INF] : osd.1 10.100.1.12:6800/1092 boot
2017-03-05 16:38:08.328543 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e28: 3 osds: 3 up, 3 in
2017-03-05 16:38:08.331187 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v47: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:08.912927 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01} v 0) v1
2017-03-05 16:38:08.912981 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34105 :/0' entity='osd.1' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node2", "root=default"], "id": 1, "weight": 0.01}]: dispatch
2017-03-05 16:38:08.913117 7f744aeee700  0 mon.mon1@0(leader).osd e28 create-or-move crush item name 'osd.1' initial_weight 0.01 at location {host=node2,root=default}
2017-03-05 16:38:09.280035 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:09.280090 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34108 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:09.280196 7f744aeee700  0 mon.mon1@0(leader).osd e28 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:09.338404 7f744c8f3700  1 mon.mon1@0(leader).osd e29 e29: 3 osds: 3 up, 3 in
2017-03-05 16:38:09.340289 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e29: 3 osds: 3 up, 3 in
2017-03-05 16:38:09.344222 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v48: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:11.682559 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:11.682606 7f744aeee700  0 log_channel(audit) log [INF] : from='client.? 10.100.1.11:0/1001155' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:11.682718 7f744aeee700  0 mon.mon1@0(leader).osd e29 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:12.747085 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v49: 64 pgs: 28 peering, 36 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:12.754297 7f744c8f3700  1 mon.mon1@0(leader).osd e30 e30: 3 osds: 2 up, 3 in
2017-03-05 16:38:12.756862 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e30: 3 osds: 2 up, 3 in
2017-03-05 16:38:12.762629 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v50: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:13.174165 7f744aeee700  0 mon.mon1@0(leader) e1 handle_command mon_command({"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01} v 0) v1
2017-03-05 16:38:13.174219 7f744aeee700  0 log_channel(audit) log [INF] : from='client.34115 :/0' entity='osd.0' cmd=[{"prefix": "osd crush create-or-move", "args": ["host=node1", "root=default"], "id": 0, "weight": 0.01}]: dispatch
2017-03-05 16:38:13.174373 7f744aeee700  0 mon.mon1@0(leader).osd e30 create-or-move crush item name 'osd.0' initial_weight 0.01 at location {host=node1,root=default}
2017-03-05 16:38:13.758454 7f744c8f3700  1 mon.mon1@0(leader).osd e31 e31: 3 osds: 3 up, 3 in
2017-03-05 16:38:13.759440 7f744c8f3700  0 log_channel(cluster) log [INF] : osd.0 10.100.1.11:6800/1031 boot
2017-03-05 16:38:13.760752 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e31: 3 osds: 3 up, 3 in
2017-03-05 16:38:13.764511 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v51: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:14.770674 7f744c8f3700  1 mon.mon1@0(leader).osd e32 e32: 3 osds: 3 up, 3 in
2017-03-05 16:38:14.772459 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e32: 3 osds: 3 up, 3 in
2017-03-05 16:38:14.776454 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v52: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:17.395005 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v53: 64 pgs: 41 peering, 23 active+clean; 0 bytes data, 102096 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:19.567219 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v54: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102364 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:44.922006 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:38:45.248089 7f744b6ef700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 30 pgs peering; 30 pgs stuck inactive; 30 pgs stuck unclean
2017-03-05 16:39:44.922500 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:40:19.605507 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v55: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:40:44.922977 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:41:44.923509 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:42:44.924059 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:43:44.924537 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:44:44.924979 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:45:44.925390 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:46:44.925822 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:47:44.926248 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:48:44.926734 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:49:44.927166 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:50:44.927613 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:51:44.927992 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:52:44.928358 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1710 MB, avail 4423 MB
2017-03-05 16:53:44.928808 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:54:44.929234 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:55:44.929688 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:56:44.930171 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:57:44.930680 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:58:44.931172 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:58:50.056058 7f744b6ef700  0 log_channel(cluster) log [INF] : osd.2 marked down after no pg stats for 900.058900seconds
2017-03-05 16:58:50.056127 7f744b6ef700 -1 mon.mon1@0(leader).osd e32 no osd or pg stats from osd.2 since 2017-03-05 16:43:49.997129, 900.058900 seconds ago.  marking down
2017-03-05 16:58:50.063761 7f744c8f3700  1 mon.mon1@0(leader).osd e33 e33: 3 osds: 2 up, 3 in
2017-03-05 16:58:50.065987 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e33: 3 osds: 2 up, 3 in
2017-03-05 16:58:50.071303 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v56: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:51.076694 7f744c8f3700  1 mon.mon1@0(leader).osd e34 e34: 3 osds: 2 up, 3 in
2017-03-05 16:58:51.078777 7f744c8f3700  0 log_channel(cluster) log [INF] : osdmap e34: 3 osds: 2 up, 3 in
2017-03-05 16:58:51.086914 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v57: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:55.822719 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v58: 64 pgs: 21 active+undersized+degraded, 6 stale+active+clean, 17 peering, 20 active+clean; 0 bytes data, 102372 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:58:56.837200 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v59: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:59:44.931709 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 16:59:45.255958 7f744b6ef700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 44 pgs degraded; 44 pgs stuck unclean; 44 pgs undersized; too few PGs per OSD (28 < min 30); 1/3 in osds are down
2017-03-05 17:00:00.000383 7f744b6ef700  0 log_channel(cluster) log [INF] : HEALTH_WARN; 44 pgs degraded; 44 pgs stuck unclean; 44 pgs undersized; too few PGs per OSD (28 < min 30); 1/3 in osds are down
2017-03-05 17:00:44.932290 7f744b6ef700  0 mon.mon1@0(leader).data_health(14) update_stats avail 72% total 6134 MB, used 1711 MB, avail 4422 MB
2017-03-05 17:00:55.858997 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v60: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 17:00:56.871368 7f744c8f3700  0 log_channel(cluster) log [INF] : pgmap v61: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
[root@mon1 ~]# cat /var/log/ceph/ceph
ceph.audit.log     ceph.log           ceph-mon.mon1.log  
[root@mon1 ~]# cat /var/log/ceph/ceph.log 
2017-03-05 15:19:36.192512 unknown.0 :/0 0 :  [INF] mkfs 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
2017-03-05 15:19:20.679023 mon.0 10.100.2.11:6789/0 15 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:25.682043 mon.0 10.100.2.11:6789/0 24 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:25.896798 mon.0 10.100.2.11:6789/0 25 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:30.900098 mon.0 10.100.2.11:6789/0 36 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:35.913850 mon.0 10.100.2.11:6789/0 47 : cluster [INF] mon.mon1@0 won leader election with quorum 0,2
2017-03-05 15:19:35.947786 mon.0 10.100.2.11:6789/0 48 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:35.976997 mon.0 10.100.2.11:6789/0 49 : cluster [INF] mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:36.075822 mon.0 10.100.2.11:6789/0 52 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:19:36.123329 mon.0 10.100.2.11:6789/0 53 : cluster [INF] mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:19:36.136865 mon.0 10.100.2.11:6789/0 54 : cluster [INF] HEALTH_ERR; no osds
2017-03-05 15:19:36.187040 mon.0 10.100.2.11:6789/0 55 : cluster [INF] monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:19:36.236912 mon.0 10.100.2.11:6789/0 56 : cluster [INF] pgmap v1: 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:19:36.305389 mon.0 10.100.2.11:6789/0 57 : cluster [INF] mdsmap e1: 0/0/0 up
2017-03-05 15:19:36.312210 mon.0 10.100.2.11:6789/0 58 : cluster [INF] osdmap e1: 0 osds: 0 up, 0 in
2017-03-05 15:19:36.377885 mon.0 10.100.2.11:6789/0 59 : cluster [INF] pgmap v2: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:20:36.137181 mon.0 10.100.2.11:6789/0 78 : cluster [INF] HEALTH_ERR; 64 pgs stuck inactive; 64 pgs stuck unclean; no osds
2017-03-05 15:26:13.375171 mon.0 10.100.2.11:6789/0 81 : cluster [INF] osdmap e2: 1 osds: 0 up, 0 in
2017-03-05 15:26:13.378885 mon.0 10.100.2.11:6789/0 82 : cluster [INF] pgmap v3: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:16.463456 mon.0 10.100.2.11:6789/0 87 : cluster [INF] osdmap e3: 1 osds: 0 up, 0 in
2017-03-05 15:26:16.466952 mon.0 10.100.2.11:6789/0 88 : cluster [INF] pgmap v4: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:17.479801 mon.0 10.100.2.11:6789/0 89 : cluster [INF] osd.0 10.100.1.11:6800/9152 boot
2017-03-05 15:26:17.481321 mon.0 10.100.2.11:6789/0 90 : cluster [INF] osdmap e4: 1 osds: 1 up, 1 in
2017-03-05 15:26:17.485170 mon.0 10.100.2.11:6789/0 91 : cluster [INF] pgmap v5: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:18.498445 mon.0 10.100.2.11:6789/0 92 : cluster [INF] osdmap e5: 1 osds: 1 up, 1 in
2017-03-05 15:26:18.503442 mon.0 10.100.2.11:6789/0 93 : cluster [INF] pgmap v6: 64 pgs: 64 creating; 0 bytes data, 0 kB used, 0 kB / 0 kB avail
2017-03-05 15:26:22.964013 mon.0 10.100.2.11:6789/0 94 : cluster [INF] pgmap v7: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:24.990546 mon.0 10.100.2.11:6789/0 97 : cluster [INF] osdmap e6: 2 osds: 1 up, 1 in
2017-03-05 15:26:24.994280 mon.0 10.100.2.11:6789/0 98 : cluster [INF] pgmap v8: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:28.015181 mon.0 10.100.2.11:6789/0 103 : cluster [INF] osdmap e7: 2 osds: 1 up, 1 in
2017-03-05 15:26:28.021432 mon.0 10.100.2.11:6789/0 104 : cluster [INF] pgmap v9: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:29.038855 mon.0 10.100.2.11:6789/0 105 : cluster [INF] osd.1 10.100.1.12:6800/9137 boot
2017-03-05 15:26:29.040547 mon.0 10.100.2.11:6789/0 106 : cluster [INF] osdmap e8: 2 osds: 2 up, 2 in
2017-03-05 15:26:29.049317 mon.0 10.100.2.11:6789/0 107 : cluster [INF] pgmap v10: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:30.057327 mon.0 10.100.2.11:6789/0 108 : cluster [INF] osdmap e9: 2 osds: 2 up, 2 in
2017-03-05 15:26:30.063750 mon.0 10.100.2.11:6789/0 109 : cluster [INF] pgmap v11: 64 pgs: 64 active+undersized+degraded; 0 bytes data, 33832 kB used, 9171 MB / 9204 MB avail
2017-03-05 15:26:34.635217 mon.0 10.100.2.11:6789/0 110 : cluster [INF] pgmap v12: 64 pgs: 31 active+undersized+degraded, 33 active+clean; 0 bytes data, 68064 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:35.651450 mon.0 10.100.2.11:6789/0 111 : cluster [INF] pgmap v13: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:36.140453 mon.0 10.100.2.11:6789/0 113 : cluster [INF] HEALTH_OK
2017-03-05 15:26:36.660937 mon.0 10.100.2.11:6789/0 115 : cluster [INF] osdmap e10: 3 osds: 2 up, 2 in
2017-03-05 15:26:36.667964 mon.0 10.100.2.11:6789/0 116 : cluster [INF] pgmap v14: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:39.695082 mon.0 10.100.2.11:6789/0 122 : cluster [INF] osdmap e11: 3 osds: 2 up, 2 in
2017-03-05 15:26:39.701779 mon.0 10.100.2.11:6789/0 123 : cluster [INF] pgmap v15: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:40.700953 mon.0 10.100.2.11:6789/0 124 : cluster [INF] osd.2 10.100.1.13:6800/9177 boot
2017-03-05 15:26:40.702993 mon.0 10.100.2.11:6789/0 125 : cluster [INF] osdmap e12: 3 osds: 3 up, 3 in
2017-03-05 15:26:40.712107 mon.0 10.100.2.11:6789/0 126 : cluster [INF] pgmap v16: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:41.721316 mon.0 10.100.2.11:6789/0 127 : cluster [INF] osdmap e13: 3 osds: 3 up, 3 in
2017-03-05 15:26:41.726748 mon.0 10.100.2.11:6789/0 128 : cluster [INF] pgmap v17: 64 pgs: 64 active+clean; 0 bytes data, 68316 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:45.966387 mon.0 10.100.2.11:6789/0 129 : cluster [INF] pgmap v18: 64 pgs: 64 active+clean; 0 bytes data, 68500 kB used, 18343 MB / 18409 MB avail
2017-03-05 15:26:46.983976 mon.0 10.100.2.11:6789/0 130 : cluster [INF] pgmap v19: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:28:46.011597 mon.0 10.100.2.11:6789/0 131 : cluster [INF] pgmap v20: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:28:47.017423 mon.0 10.100.2.11:6789/0 132 : cluster [INF] pgmap v21: 64 pgs: 64 active+clean; 0 bytes data, 102032 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:31:01.556840 mon.0 10.100.2.11:6789/0 133 : cluster [INF] osd.1 marked itself down
2017-03-05 15:31:01.556840 mon.0 10.100.2.11:6789/0 133 : cluster [INF] osd.1 marked itself down
2017-03-05 15:49:30.112877 mon.1 10.100.2.12:6789/0 80 : cluster [INF] osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:30.128461 mon.1 10.100.2.12:6789/0 81 : cluster [INF] pgmap v32: 64 pgs: 9 active+undersized+degraded, 14 stale+active+clean, 30 peering, 11 active+clean; 0 bytes data, 101296 kB used, 27516 MB / 27614 MB avail
2017-03-05 15:49:31.083706 mon.0 10.100.2.11:6789/0 1 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:49:31.088939 mon.0 10.100.2.11:6789/0 2 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 15:49:31.090137 mon.0 10.100.2.11:6789/0 3 : cluster [INF] mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 15:49:31.093506 mon.0 10.100.2.11:6789/0 4 : cluster [INF] HEALTH_WARN; 9 pgs degraded; 30 pgs peering; 14 pgs stale; 30 pgs stuck inactive; 39 pgs stuck unclean; 9 pgs undersized
2017-03-05 15:49:31.095864 mon.0 10.100.2.11:6789/0 5 : cluster [WRN] mon.1 10.100.2.12:6789/0 clock skew 0.954216s > max 0.05s
2017-03-05 15:49:31.095947 mon.0 10.100.2.11:6789/0 6 : cluster [WRN] mon.2 10.100.2.13:6789/0 clock skew 0.573805s > max 0.05s
2017-03-05 15:49:31.098344 mon.0 10.100.2.11:6789/0 7 : cluster [INF] monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 15:49:31.098420 mon.0 10.100.2.11:6789/0 8 : cluster [INF] pgmap v32: 64 pgs: 9 active+undersized+degraded, 14 stale+active+clean, 30 peering, 11 active+clean; 0 bytes data, 101296 kB used, 27516 MB / 27614 MB avail
2017-03-05 15:49:31.098534 mon.0 10.100.2.11:6789/0 9 : cluster [INF] mdsmap e1: 0/0/0 up
2017-03-05 15:49:31.098673 mon.0 10.100.2.11:6789/0 10 : cluster [INF] osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 15:49:31.100525 mon.0 10.100.2.11:6789/0 11 : cluster [WRN] message from mon.1 was stamped 0.957313s in the future, clocks not synchronized
2017-03-05 15:49:31.663562 mon.2 10.100.2.13:6789/0 18 : cluster [INF] mon.mon3 calling new monitor election
2017-03-05 15:49:32.043787 mon.1 10.100.2.12:6789/0 82 : cluster [INF] mon.mon2 calling new monitor election
2017-03-05 15:49:32.180972 mon.0 10.100.2.11:6789/0 14 : cluster [INF] pgmap v33: 64 pgs: 9 active+undersized+degraded, 30 peering, 25 active+clean; 0 bytes data, 101888 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:49:34.204493 mon.0 10.100.2.11:6789/0 15 : cluster [INF] pgmap v34: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 15:49:38.947096 mon.0 10.100.2.11:6789/0 16 : cluster [WRN] message from mon.1 was stamped 0.382939s in the future, clocks not synchronized
2017-03-05 15:50:06.009458 mon.0 10.100.2.11:6789/0 17 : cluster [WRN] message from mon.1 was stamped 0.379459s in the future, clocks not synchronized
2017-03-05 15:50:31.093311 mon.0 10.100.2.11:6789/0 18 : cluster [INF] HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 15:51:32.453102 mon.0 10.100.2.11:6789/0 19 : cluster [INF] pgmap v35: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:34.464605 mon.0 10.100.2.11:6789/0 20 : cluster [INF] pgmap v36: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:51:35.476599 mon.0 10.100.2.11:6789/0 21 : cluster [INF] pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 15:52:12.489547 mon.0 10.100.2.11:6789/0 22 : cluster [WRN] message from mon.1 was stamped 0.380443s in the future, clocks not synchronized
2017-03-05 15:54:31.094988 mon.0 10.100.2.11:6789/0 23 : cluster [WRN] mon.1 10.100.2.12:6789/0 clock skew 0.37869s > max 0.05s
2017-03-05 16:00:00.000313 mon.0 10.100.2.11:6789/0 24 : cluster [INF] HEALTH_WARN; clock skew detected on mon.mon2, mon.mon3; Monitor clock skew detected 
2017-03-05 16:17:18.504450 mon.0 10.100.2.11:6789/0 3 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 16:17:18.515743 mon.2 10.100.2.13:6789/0 1 : cluster [INF] mon.mon3 calling new monitor election
2017-03-05 16:17:18.955964 mon.0 10.100.2.11:6789/0 6 : cluster [INF] mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:17:18.960940 mon.0 10.100.2.11:6789/0 7 : cluster [INF] HEALTH_OK
2017-03-05 16:17:18.961657 mon.1 10.100.2.12:6789/0 1 : cluster [INF] mon.mon2 calling new monitor election
2017-03-05 16:17:19.002648 mon.0 10.100.2.11:6789/0 8 : cluster [INF] monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:17:19.002758 mon.0 10.100.2.11:6789/0 9 : cluster [INF] pgmap v37: 64 pgs: 64 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:19.002882 mon.0 10.100.2.11:6789/0 10 : cluster [INF] mdsmap e1: 0/0/0 up
2017-03-05 16:17:19.003122 mon.0 10.100.2.11:6789/0 11 : cluster [INF] osdmap e22: 3 osds: 3 up, 3 in
2017-03-05 16:17:19.003626 mon.0 10.100.2.11:6789/0 12 : cluster [WRN] mon.1 10.100.2.12:6789/0 clock skew 0.0656535s > max 0.05s
2017-03-05 16:17:19.012172 mon.0 10.100.2.11:6789/0 13 : cluster [WRN] message from mon.1 was stamped 0.110942s in the future, clocks not synchronized
2017-03-05 16:17:23.621660 mon.0 10.100.2.11:6789/0 19 : cluster [INF] osdmap e23: 3 osds: 2 up, 3 in
2017-03-05 16:17:23.627956 mon.0 10.100.2.11:6789/0 20 : cluster [INF] pgmap v38: 64 pgs: 14 stale+active+clean, 50 active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:24.632640 mon.0 10.100.2.11:6789/0 24 : cluster [WRN] message from mon.1 was stamped 0.108390s in the future, clocks not synchronized
2017-03-05 16:17:24.635557 mon.0 10.100.2.11:6789/0 25 : cluster [INF] osd.2 10.100.1.13:6800/1135 boot
2017-03-05 16:17:24.640694 mon.0 10.100.2.11:6789/0 26 : cluster [INF] osdmap e24: 3 osds: 1 up, 3 in
2017-03-05 16:17:24.646839 mon.0 10.100.2.11:6789/0 27 : cluster [INF] pgmap v39: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:25.644805 mon.0 10.100.2.11:6789/0 31 : cluster [INF] osd.1 10.100.1.12:6800/1049 boot
2017-03-05 16:17:25.644923 mon.0 10.100.2.11:6789/0 32 : cluster [INF] osd.0 10.100.1.11:6800/1078 boot
2017-03-05 16:17:25.647187 mon.0 10.100.2.11:6789/0 33 : cluster [INF] osdmap e25: 3 osds: 3 up, 3 in
2017-03-05 16:17:25.651241 mon.0 10.100.2.11:6789/0 34 : cluster [INF] pgmap v40: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:26.659198 mon.0 10.100.2.11:6789/0 38 : cluster [INF] osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:17:26.667816 mon.0 10.100.2.11:6789/0 39 : cluster [INF] pgmap v41: 64 pgs: 64 stale+active+clean; 0 bytes data, 102288 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:17:28.722715 mon.0 10.100.2.11:6789/0 40 : cluster [INF] pgmap v42: 64 pgs: 50 stale+active+clean, 14 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:29.738146 mon.0 10.100.2.11:6789/0 41 : cluster [INF] pgmap v43: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:17:51.746827 mon.0 10.100.2.11:6789/0 42 : cluster [WRN] message from mon.1 was stamped 0.098338s in the future, clocks not synchronized
2017-03-05 16:18:18.960791 mon.0 10.100.2.11:6789/0 43 : cluster [INF] HEALTH_WARN; clock skew detected on mon.mon2; Monitor clock skew detected 
2017-03-05 16:19:28.764202 mon.0 10.100.2.11:6789/0 44 : cluster [INF] pgmap v44: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:29.775961 mon.0 10.100.2.11:6789/0 45 : cluster [INF] pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:19:57.785443 mon.0 10.100.2.11:6789/0 46 : cluster [WRN] message from mon.1 was stamped 0.059999s in the future, clocks not synchronized
2017-03-05 16:27:18.964074 mon.0 10.100.2.11:6789/0 48 : cluster [INF] HEALTH_OK
2017-03-05 16:27:18.964074 mon.0 10.100.2.11:6789/0 48 : cluster [INF] HEALTH_OK
2017-03-05 16:28:45.129656 mon.0 10.100.2.11:6789/0 1 : cluster [INF] mon.mon1 calling new monitor election
2017-03-05 16:28:45.136872 mon.1 10.100.2.12:6789/0 1 : cluster [INF] mon.mon2 calling new monitor election
2017-03-05 16:28:45.146034 mon.2 10.100.2.13:6789/0 1 : cluster [INF] mon.mon3 calling new monitor election
2017-03-05 16:28:45.214732 mon.0 10.100.2.11:6789/0 2 : cluster [INF] mon.mon1@0 won leader election with quorum 0,1,2
2017-03-05 16:28:45.244810 mon.0 10.100.2.11:6789/0 3 : cluster [INF] HEALTH_OK
2017-03-05 16:28:45.318531 mon.0 10.100.2.11:6789/0 4 : cluster [INF] monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
2017-03-05 16:28:45.318651 mon.0 10.100.2.11:6789/0 5 : cluster [INF] pgmap v45: 64 pgs: 64 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:28:45.318771 mon.0 10.100.2.11:6789/0 6 : cluster [INF] mdsmap e1: 0/0/0 up
2017-03-05 16:28:45.319040 mon.0 10.100.2.11:6789/0 7 : cluster [INF] osdmap e26: 3 osds: 3 up, 3 in
2017-03-05 16:38:07.315491 mon.0 10.100.2.11:6789/0 16 : cluster [INF] osdmap e27: 3 osds: 2 up, 3 in
2017-03-05 16:38:07.329139 mon.0 10.100.2.11:6789/0 17 : cluster [INF] pgmap v46: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:08.327206 mon.0 10.100.2.11:6789/0 19 : cluster [INF] osd.1 10.100.1.12:6800/1092 boot
2017-03-05 16:38:08.328545 mon.0 10.100.2.11:6789/0 20 : cluster [INF] osdmap e28: 3 osds: 3 up, 3 in
2017-03-05 16:38:08.331190 mon.0 10.100.2.11:6789/0 21 : cluster [INF] pgmap v47: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:09.340293 mon.0 10.100.2.11:6789/0 24 : cluster [INF] osdmap e29: 3 osds: 3 up, 3 in
2017-03-05 16:38:09.344226 mon.0 10.100.2.11:6789/0 25 : cluster [INF] pgmap v48: 64 pgs: 28 stale+active+clean, 36 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:12.747090 mon.0 10.100.2.11:6789/0 27 : cluster [INF] pgmap v49: 64 pgs: 28 peering, 36 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:12.756865 mon.0 10.100.2.11:6789/0 28 : cluster [INF] osdmap e30: 3 osds: 2 up, 3 in
2017-03-05 16:38:12.762634 mon.0 10.100.2.11:6789/0 29 : cluster [INF] pgmap v50: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:13.759446 mon.0 10.100.2.11:6789/0 31 : cluster [INF] osd.0 10.100.1.11:6800/1031 boot
2017-03-05 16:38:13.760754 mon.0 10.100.2.11:6789/0 32 : cluster [INF] osdmap e31: 3 osds: 3 up, 3 in
2017-03-05 16:38:13.764515 mon.0 10.100.2.11:6789/0 33 : cluster [INF] pgmap v51: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:14.772461 mon.0 10.100.2.11:6789/0 34 : cluster [INF] osdmap e32: 3 osds: 3 up, 3 in
2017-03-05 16:38:14.776457 mon.0 10.100.2.11:6789/0 35 : cluster [INF] pgmap v52: 64 pgs: 22 stale+active+clean, 28 peering, 14 active+clean; 0 bytes data, 102280 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:17.395009 mon.0 10.100.2.11:6789/0 36 : cluster [INF] pgmap v53: 64 pgs: 41 peering, 23 active+clean; 0 bytes data, 102096 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:38:19.567223 mon.0 10.100.2.11:6789/0 37 : cluster [INF] pgmap v54: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102364 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:38:45.248102 mon.0 10.100.2.11:6789/0 38 : cluster [INF] HEALTH_WARN; 30 pgs peering; 30 pgs stuck inactive; 30 pgs stuck unclean
2017-03-05 16:40:19.605511 mon.0 10.100.2.11:6789/0 39 : cluster [INF] pgmap v55: 64 pgs: 30 peering, 34 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:50.056079 mon.0 10.100.2.11:6789/0 40 : cluster [INF] osd.2 marked down after no pg stats for 900.058900seconds
2017-03-05 16:58:50.065990 mon.0 10.100.2.11:6789/0 41 : cluster [INF] osdmap e33: 3 osds: 2 up, 3 in
2017-03-05 16:58:50.071307 mon.0 10.100.2.11:6789/0 42 : cluster [INF] pgmap v56: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:51.078783 mon.0 10.100.2.11:6789/0 43 : cluster [INF] osdmap e34: 3 osds: 2 up, 3 in
2017-03-05 16:58:51.086918 mon.0 10.100.2.11:6789/0 44 : cluster [INF] pgmap v57: 64 pgs: 14 stale+active+clean, 30 peering, 20 active+clean; 0 bytes data, 102224 kB used, 27515 MB / 27614 MB avail
2017-03-05 16:58:55.822723 mon.0 10.100.2.11:6789/0 45 : cluster [INF] pgmap v58: 64 pgs: 21 active+undersized+degraded, 6 stale+active+clean, 17 peering, 20 active+clean; 0 bytes data, 102372 kB used, 27514 MB / 27614 MB avail
2017-03-05 16:58:56.837203 mon.0 10.100.2.11:6789/0 46 : cluster [INF] pgmap v59: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 16:59:45.255986 mon.0 10.100.2.11:6789/0 47 : cluster [INF] HEALTH_WARN; 44 pgs degraded; 44 pgs stuck unclean; 44 pgs undersized; too few PGs per OSD (28 < min 30); 1/3 in osds are down
2017-03-05 17:00:00.000406 mon.0 10.100.2.11:6789/0 48 : cluster [INF] HEALTH_WARN; 44 pgs degraded; 44 pgs stuck unclean; 44 pgs undersized; too few PGs per OSD (28 < min 30); 1/3 in osds are down
2017-03-05 17:00:55.859065 mon.0 10.100.2.11:6789/0 49 : cluster [INF] pgmap v60: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
2017-03-05 17:00:56.871373 mon.0 10.100.2.11:6789/0 50 : cluster [INF] pgmap v61: 64 pgs: 44 active+undersized+degraded, 20 active+clean; 0 bytes data, 100 MB used, 27514 MB / 27614 MB avail
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# ceph -s
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_WARN
            44 pgs degraded
            44 pgs stuck unclean
            44 pgs undersized
            too few PGs per OSD (28 < min 30)
            1/3 in osds are down
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e34: 3 osds: 2 up, 3 in
      pgmap v61: 64 pgs, 1 pools, 0 bytes data, 0 objects
            100 MB used, 27514 MB / 27614 MB avail
                  44 active+undersized+degraded
                  20 active+clean
[root@mon1 ~]# ceph -s
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_WARN
            44 pgs degraded
            44 pgs stuck unclean
            44 pgs undersized
            too few PGs per OSD (28 < min 30)
            1/3 in osds are down
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e34: 3 osds: 2 up, 3 in
      pgmap v61: 64 pgs, 1 pools, 0 bytes data, 0 objects
            100 MB used, 27514 MB / 27614 MB avail
                  44 active+undersized+degraded
                  20 active+clean
[root@mon1 ~]# ceph health detail
HEALTH_WARN 44 pgs degraded; 44 pgs stuck unclean; 44 pgs undersized; too few PGs per OSD (28 < min 30); 1/3 in osds are down
pg 0.29 is stuck unclean for 4423.540711, current state active+undersized+degraded, last acting [0]
pg 0.24 is stuck unclean for 4423.540838, current state active+undersized+degraded, last acting [0]
pg 0.23 is stuck unclean for 2746.155751, current state active+undersized+degraded, last acting [1]
pg 0.22 is stuck unclean for 4423.523675, current state active+undersized+degraded, last acting [1]
pg 0.21 is stuck unclean for 2746.155916, current state active+undersized+degraded, last acting [1]
pg 0.1f is stuck unclean for 4423.541019, current state active+undersized+degraded, last acting [0]
pg 0.1c is stuck unclean for 2746.156147, current state active+undersized+degraded, last acting [1]
pg 0.1b is stuck unclean for 2746.156338, current state active+undersized+degraded, last acting [1]
pg 0.19 is stuck unclean for 2746.138459, current state active+undersized+degraded, last acting [0]
pg 0.18 is stuck unclean for 4423.539896, current state active+undersized+degraded, last acting [0]
pg 0.16 is stuck unclean for 4423.517058, current state active+undersized+degraded, last acting [1]
pg 0.15 is stuck unclean for 2746.138621, current state active+undersized+degraded, last acting [0]
pg 0.14 is stuck unclean for 2746.138751, current state active+undersized+degraded, last acting [0]
pg 0.13 is stuck unclean for 2746.153450, current state active+undersized+degraded, last acting [1]
pg 0.12 is stuck unclean for 2746.153629, current state active+undersized+degraded, last acting [1]
pg 0.11 is stuck unclean for 2746.153777, current state active+undersized+degraded, last acting [1]
pg 0.3f is stuck unclean for 2746.144752, current state active+undersized+degraded, last acting [0]
pg 0.10 is stuck unclean for 2746.138892, current state active+undersized+degraded, last acting [0]
pg 0.3e is stuck unclean for 4423.540000, current state active+undersized+degraded, last acting [0]
pg 0.f is stuck unclean for 2746.144872, current state active+undersized+degraded, last acting [0]
pg 0.3d is stuck unclean for 2746.144988, current state active+undersized+degraded, last acting [0]
pg 0.d is stuck unclean for 4423.517165, current state active+undersized+degraded, last acting [1]
pg 0.3b is stuck unclean for 2746.145123, current state active+undersized+degraded, last acting [0]
pg 0.c is stuck unclean for 2746.153922, current state active+undersized+degraded, last acting [1]
pg 0.3a is stuck unclean for 2746.145315, current state active+undersized+degraded, last acting [0]
pg 0.a is stuck unclean for 4423.517262, current state active+undersized+degraded, last acting [1]
pg 0.8 is stuck unclean for 2746.154059, current state active+undersized+degraded, last acting [1]
pg 0.36 is stuck unclean for 2746.154247, current state active+undersized+degraded, last acting [1]
pg 0.7 is stuck unclean for 2746.145555, current state active+undersized+degraded, last acting [0]
pg 0.35 is stuck unclean for 2746.154481, current state active+undersized+degraded, last acting [1]
pg 0.6 is stuck unclean for 2746.145729, current state active+undersized+degraded, last acting [0]
pg 0.34 is stuck unclean for 2746.154612, current state active+undersized+degraded, last acting [1]
pg 0.5 is stuck unclean for 4423.540207, current state active+undersized+degraded, last acting [0]
pg 0.33 is stuck unclean for 2746.154966, current state active+undersized+degraded, last acting [1]
pg 0.4 is stuck unclean for 2746.154757, current state active+undersized+degraded, last acting [1]
pg 0.32 is stuck unclean for 2746.145868, current state active+undersized+degraded, last acting [0]
pg 0.31 is stuck unclean for 2746.145996, current state active+undersized+degraded, last acting [0]
pg 0.30 is stuck unclean for 4423.540452, current state active+undersized+degraded, last acting [0]
pg 0.1 is stuck unclean for 2746.155113, current state active+undersized+degraded, last acting [1]
pg 0.2f is stuck unclean for 4423.517437, current state active+undersized+degraded, last acting [1]
pg 0.2d is stuck unclean for 2746.155475, current state active+undersized+degraded, last acting [1]
pg 0.2c is stuck unclean for 4423.540673, current state active+undersized+degraded, last acting [0]
pg 0.2b is stuck unclean for 2746.155666, current state active+undersized+degraded, last acting [1]
pg 0.2a is stuck unclean for 4423.517596, current state active+undersized+degraded, last acting [1]
pg 0.22 is active+undersized+degraded, acting [1]
pg 0.21 is active+undersized+degraded, acting [1]
pg 0.1f is active+undersized+degraded, acting [0]
pg 0.1c is active+undersized+degraded, acting [1]
pg 0.1b is active+undersized+degraded, acting [1]
pg 0.19 is active+undersized+degraded, acting [0]
pg 0.18 is active+undersized+degraded, acting [0]
pg 0.16 is active+undersized+degraded, acting [1]
pg 0.15 is active+undersized+degraded, acting [0]
pg 0.14 is active+undersized+degraded, acting [0]
pg 0.13 is active+undersized+degraded, acting [1]
pg 0.12 is active+undersized+degraded, acting [1]
pg 0.11 is active+undersized+degraded, acting [1]
pg 0.10 is active+undersized+degraded, acting [0]
pg 0.f is active+undersized+degraded, acting [0]
pg 0.d is active+undersized+degraded, acting [1]
pg 0.c is active+undersized+degraded, acting [1]
pg 0.a is active+undersized+degraded, acting [1]
pg 0.8 is active+undersized+degraded, acting [1]
pg 0.7 is active+undersized+degraded, acting [0]
pg 0.6 is active+undersized+degraded, acting [0]
pg 0.5 is active+undersized+degraded, acting [0]
pg 0.4 is active+undersized+degraded, acting [1]
pg 0.1 is active+undersized+degraded, acting [1]
pg 0.3f is active+undersized+degraded, acting [0]
pg 0.3e is active+undersized+degraded, acting [0]
pg 0.3d is active+undersized+degraded, acting [0]
pg 0.3b is active+undersized+degraded, acting [0]
pg 0.3a is active+undersized+degraded, acting [0]
pg 0.36 is active+undersized+degraded, acting [1]
pg 0.35 is active+undersized+degraded, acting [1]
pg 0.34 is active+undersized+degraded, acting [1]
pg 0.33 is active+undersized+degraded, acting [1]
pg 0.32 is active+undersized+degraded, acting [0]
pg 0.31 is active+undersized+degraded, acting [0]
pg 0.30 is active+undersized+degraded, acting [0]
pg 0.2f is active+undersized+degraded, acting [1]
pg 0.2d is active+undersized+degraded, acting [1]
pg 0.2c is active+undersized+degraded, acting [0]
pg 0.2b is active+undersized+degraded, acting [1]
pg 0.2a is active+undersized+degraded, acting [1]
pg 0.29 is active+undersized+degraded, acting [0]
pg 0.24 is active+undersized+degraded, acting [0]
pg 0.23 is active+undersized+degraded, acting [1]
too few PGs per OSD (28 < min 30)
osd.2 is down since epoch 33, last address 10.100.1.13:6800/1135
[root@mon1 ~]# ssh 10.100.1.13
Last login: Sun Mar  5 16:47:24 2017 from mgmt-a087.rhpds.opentlc.com
[root@node3 ~]# 
[root@node3 ~]# 
[root@node3 ~]# 
[root@node3 ~]# 
[root@node3 ~]# ntpstat 
synchronised to NTP server (108.61.73.244) at stratum 3 
   time correct to within 46 ms
   polling server every 64 s
[root@node3 ~]# ceph -s
2017-03-05 17:03:57.073743 7f77d79c0700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2017-03-05 17:03:57.073746 7f77d79c0700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
[root@node3 ~]# cd /etc/ceph/
[root@node3 ceph]# ls
ceph.conf  rbdmap  tmppn6Fj5
[root@node3 ceph]# exit
logout
Connection to 10.100.1.13 closed.
[root@mon1 ~]# ssh 10.100.1.12
Last login: Sun Mar  5 16:46:35 2017 from mgmt-a087.rhpds.opentlc.com
[root@node2 ~]# cd /etc/ceph/
[root@node2 ceph]# ls
ceph.conf  rbdmap  tmpSlKFjy
[root@node2 ceph]# ceph -s
2017-03-05 17:04:35.282715 7fdff3fff700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2017-03-05 17:04:35.282721 7fdff3fff700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
[root@node2 ceph]# 
[root@node2 ceph]# 
[root@node2 ceph]# 
[root@node2 ceph]# exit
logout
Connection to 10.100.1.12 closed.
[root@mon1 ~]# ssh 10.100.1.13
Last login: Sun Mar  5 17:03:44 2017 from mon1.example.com
[root@node3 ~]# ceph -s
2017-03-05 17:04:50.011989 7f5ee798c700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2017-03-05 17:04:50.011994 7f5ee798c700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
[root@node3 ~]# cd /etc/ceph/
[root@node3 ceph]# ls
ceph.conf  rbdmap  tmppn6Fj5
[root@node3 ceph]# cat ceph.conf 
[global]
fsid = 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
mon_initial_members = mon1, mon2, mon3
mon_host = 10.100.2.11,10.100.2.12,10.100.2.13
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true
osd_journal_size = 1024
osd_pool_default_size = 2
osd_pool_default_min_size = 1
mon_osd_allow_primary_affinity = 1

[root@node3 ceph]# exit
logout
Connection to 10.100.1.13 closed.
[root@mon1 ~]# ls /etc/ceph/
ceph.client.admin.keyring  ceph.conf  rbdmap  tmpuXGHSg
[root@mon1 ~]# scp /etc/ceph/ceph.client.admin.keyring 10.100.1.13:/etc/ceph/
ceph.client.admin.keyring                                                                                                                                                                                                                     100%   63     0.1KB/s   00:00    
[root@mon1 ~]# scp /etc/ceph/ceph.client.admin.keyring 10.100.1.12:/etc/ceph/
ceph.client.admin.keyring                                                                                                                                                                                                                     100%   63     0.1KB/s   00:00    
[root@mon1 ~]# scp /etc/ceph/ceph.client.admin.keyring 10.100.1.11:/etc/ceph/
ceph.client.admin.keyring                                                                                                                                                                                                                     100%   63     0.1KB/s   00:00    
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# 
[root@mon1 ~]# ceph -s
    cluster 9121ece8-fbeb-4685-bfe5-7a00cad3dce3
     health HEALTH_OK
     monmap e1: 3 mons at {mon1=10.100.2.11:6789/0,mon2=10.100.2.12:6789/0,mon3=10.100.2.13:6789/0}
            election epoch 14, quorum 0,1,2 mon1,mon2,mon3
     osdmap e36: 3 osds: 2 up, 2 in
      pgmap v65: 64 pgs, 1 pools, 0 bytes data, 0 objects
            68732 kB used, 18342 MB / 18409 MB avail
                  64 active+clean
[root@mon1 ~]# 
